[ { "title": "Hello World of Three.js", "url": "/posts/hello-world-of-three-js/", "categories": "", "tags": "", "date": "2023-12-06 15:48:00 -0600", "snippet": "This is the demo I created when learning Three.js. three.js with jekyll postI finally figured out how to embed HTML with three.js in a markdown file.The HTML file looks like this. Created a js folder inside assets, and put the javascript file inside it.&lt;style&gt;.threejs { position: relative; width: 100%; padding-top: 56.25%; /* 16:9 aspect ratio */ } .threejs &gt; * { position: absolute; top: 0; bottom: 0; left: 0; right: 0; }&lt;/style&gt;&lt;div class='threejs'&gt; &lt;div id='cube'&gt;&lt;/div&gt;&lt;/div&gt;&lt;script type = 'module' src='/assets/js/helloworldthreejs1.js'&gt;&lt;/script&gt;It’s not the optimal solution I think, because I didn’t use npm to install Three.js in this blog project. Instead, simply use the following code inside the ‘helloworldthreejs1.js’ file import * as THREE from 'https://unpkg.com/three@0.126.1/build/three.module.js'hello spinning cubesA common three.js application is to pass a Scene and a Camera to a Renderer and it renders the portion of the 3D scene that is inside the frustum of the camera as a 2D image to a canvas.Here is the structure of this demo. The demo is adapted from https://threejs.org/manual/#en/fundamentals.We have three main components in this program, Renderer, Camera, and Scene. We also need a canvas to put our rendered 2D image.get container to put the rendered 2D imageThe name of the container should be the same as the one in the HTML file. var container = document.getElementById(\"cube\"); var width = container.clientWidth; var height = container.clientHeight;create sceneOur scene contains 3 cubes with phong materials as well as a directional light source. Let’s create the scene first. const scene = new THREE.Scene();Create three cube instances using the same geometry, save material and different colors. And add each cube to the scene.const geometry = new THREE.BoxGeometry( 1, 1, 1 );function makeInstance(geometry, color, x) {\tconst material = new THREE.MeshPhongMaterial({color}); \tconst cube = new THREE.Mesh(geometry, material);\tscene.add(cube); \tcube.position.x = x; \treturn cube;}//cube instancesconst cubes = [\tmakeInstance(geometry, 0x44aa88, 0),\tmakeInstance(geometry, 0x8844aa, -2),\tmakeInstance(geometry, 0xaa8844, 2),];Create light source to make the cube 3D. And add light source to the scene.//light sourceconst color = 0xFFFFFF;const intensity = 3;const light = new THREE.DirectionalLight(color, intensity);light.position.set(-1, 2, 4);scene.add(light);create cameraconst camera = new THREE.PerspectiveCamera( 75, width / height, 0.1, 1000 );camera.position.z = 5;create renderer and add to containerWe use WebGLRenderer here.const renderer = new THREE.WebGLRenderer();renderer.setSize(width, height);container.appendChild(renderer.domElement);Loop the scenefunction animate() {\t//Requests the browser to call the animate function before the next repaint, creating a smooth animation loop.\trequestAnimationFrame( animate );\t\tcubes.forEach((cube, ndx) =&gt; {\t\tcube.rotation.x += 0.01;\t\tcube.rotation.y += 0.01;\t });\trenderer.render( scene, camera );}animate();" }, { "title": "Learn WebGL Demo", "url": "/posts/learn-webgl-demo/", "categories": "", "tags": "", "date": "2023-10-11 10:31:00 -0500", "snippet": "This is the demo I created when learning WebGL. " }, { "title": "Drawing a triangle using Vulkan (1)", "url": "/posts/drawing-a-triangle-1/", "categories": "CG_Notes", "tags": "CG", "date": "2023-09-13 14:33:00 -0500", "snippet": "After writing or copying over 1000 lines of code, I finally rendered my first triangle using Vulkan. Seeing it on the screen was so exciting!I want to learn Vulkan to study state-of-the-art techniques in the field of computer graphics. I find the process of learning new techniques enjoyable, which makes me feel refreshed and positive.Application outlineThe base code is simple. Basically four steps involved in an application:void HelloTriangleApplication::run(){ initWindow(); initVulkan(); mainLoop(); cleanUp();}initWindow()One major difference between OpenGL and Vulkan is that, for OpenGL, a window is required to create a context. If one would like to do off-screen rendering, a hidden window will be the solution. In contrast, Vulkan can function perfectly well without a window.We still need to know how to show the rendered object in a window. We use GLFW library to create a window to display the rendered image on the screen.initVulkan()Initializing Vulkan is the most verbose part of the code. Every detail related to the graphics API needs to be set up explicitly from scratch in the application. As you can see in the following code, it takes 13 steps to initialize Vulkan. We will go into detail on each part and try to understand them.void HelloTriangleApplication::initVulkan(){ createInstance(); setupDebugMessenger(); createSurface(); pickPhysicalDevice(); createLogicalDevice(); createSwapChain(); createImageViews(); createRenderPass(); createGraphicsPipeline(); createFramebuffers(); createCommandPool(); createCommandBuffer(); createSyncObjects();}mainLoop()In the main loop, we will draw a frame until the window is closed or we encountered an error.Everything in the drawFrame() are asynchronous, when we exit the loop, the operations may still be going on, thus, we need to wait for the device to finish all the operations before exiting mainLoop() and enter cleanUp().void HelloTriangleApplication::mainLoop(){ while (!glfwWindowShouldClose(window)) { glfwPollEvents(); drawFrame(); } vkDeviceWaitIdle(device);}cleanUp()Once the window is closed, everything we created need to be destroy explicitly.Vulkan InitializationcreateInstance()VkInstance instance is needed for every Vulkan application which is the connection between the application and the Vulkan library. VkInstance is the first Vulkan object we created.The general pattern to create a Vulkan object is: Pointer to struct with creation info, a struct related to the object we want to create Pointer to custom allocator callbacks, for now, idk what it is Pointer to the variable that stores the handle to the new object The function will return VkResult to tell if we create the object successfullyVulkan object created by us needs to be explicitly destroyed. vkCreateXXX + vkDestroyXXX, vkAllocateXXX + vkFreeXXXThe VkInstanceCreateInfo createInfo contains the information about the application, global extension, and enabled layers like the validation layer for debugging purpose. Call vkCreateInstance to create the Vulkan instance.VkResult result = vkCreateInstance(&amp;createInfo, nullptr, &amp;instance);The correspondence destroy function is vkDestroyInstance(instance,nullptr).Now let’s go into detail on the vkInstanceCreateInfo. Typically, Vulkan struct will need us to explicitly set the VkStructureType. The information about the application including the name of the application, version information, etc. The enabled layers information contains the layers we want to enable. Vulkan introduces validation layers system to help us debug our program. The enabled extension information contains the extensions we enabled in the program.typedef struct VkInstanceCreateInfo { VkStructureType sType; //the type of the struct need to be specified const void* pNext; //optional VkInstanceCreateFlags flags;\t\t//application information const VkApplicationInfo* pApplicationInfo;\t\t//enable layers information uint32_t enabledLayerCount; const char* const* ppEnabledLayerNames;\t\t//enable extension information uint32_t enabledExtensionCount; const char* const* ppEnabledExtensionNames;} VkInstanceCreateInfo;Extensions glfwExtensions Vulkan is a platform agnostic API, extensions are required to interface with the window system. GLFW provide built-in function that returns the required extensions. glfwExtensions = glfwGetRequiredInstanceExtensions(&amp;glfwExtensionCount); debug extensions To setup a debug messenger with a callback we need to enable the extension: VK_EXT_DEBUG_UTILS_EXTENSION_NAME solve MacOS issue VK_ERROR_INCOMPATIBLE_DRIVER VK_KHR_PORTABILITY_subset extension is mandatory(1) VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAMEAlso, a flag VK_INSTANCE_CREATE_ENUMERATE_PORTABILITY_BIT_KHR should be added to createInfo(2) “VK_KHR_get_physical_device_properties2”Note that the first extension is a macro definition, the second is a stringValidation LayersWe enabled “VK_LAYER_KHRONOS_validation” layer if in Debug mode.We also need to check if the system support validation layer.The general pattern to get properties from Vulkan is list in the following code. We will call the vkEnumerateXXXProperties for two times. The first time is to get the number of the properties, then we can initialize the container for handling the exact data.uint32_t count = 0;//the first parametervkEnumerateXXXProperties(&amp;count,nullptr);std::vector&lt;VkXXXProperties&gt; properties(count);vkEnumerateXXXProperties(&amp;count,properties.data());For checking the validation layer VkLayerProperties support, the function is vkEnumerateInstanceLayerProperties.createSurface()After creating the instance, we need to create a VkSurfaceKHR object to present rendered images to the screen. Here we use the built-in function of GLFW to create window surface.glfwCreateWindowSurface(instance, window, nullptr, &amp;surface)pickPhysicalDevice()To select a suitable physical device which is a graphics card in the system. We are able to select any number of graphics cards and use them simultaneously. For beginner, we only use the first graphic card ): looks like I have choice, developing on MacOS.To get available physical devices VkPhysicalDevice, is similar to get properties except we need the instance. The function isvkEnumeratePhysicalDevices(instance, &amp;deviceCount, devices.data());From the available physical devices, we will choose the one suitable for our application.isDeviceSuitable(device)The suitable device will satisfy the following three requirements: queue families, extensions, swap chain findQueueFamilies(device)Most operations performed with Vulkan from draw commands to uploading textures requires commands to be submitted to a queue. Different types of queues are from different queue families which support different subset of commands. We need to find out the queue families that are supported by the device as well as meet our requirements. In this simply triangle application, we need the indices of two queue families includes graphics family that supports graphics commands and present family that supports presenting the rendered image to the surface. To get the supported queue families VkQueueFamilyProperties is similar to get properties as mentioned before.vkGetPhysicalDeviceQueueFamilyProperties(device, &amp;queueFamilyCount, queueFamilies.data())Then we iterate over the queue families we get and find out the index of queue we needed. checkDeviceExtensionSupport(device)We first get all the available extensions, then iterate over the extensions to check if the required extensions exist. Currently, the required device extension is VK_KHR_SWAPCHAIN_EXTENSION_NAMEvkEnumerateDeviceExtensionProperties(device, nullptr, &amp;extensionCount, availableExtensions.data()); check swap chain supportSwap chain is like a “default framebuffer” to store the images waiting to be presented to the screen. If swap chain extension in 2 is supported, we need to query the detail of the swap chain. And check if it meet our requirement. In this simple program, we will consider the swap chain adequate if it supports at least one surface format(pixel format, color space) and one presentation mode.If we find the queue families indices and all the extensions supported, we will choose this device as our physical device.createLogicalDevice()A logical device is needed to set up to interface with the physical device we just created where we can describe more specifically how we will use the physical device.We need to specifying the queues to be created, the device features to be used, the extensions to be enabled, and the validation layer to be enabled.To solve the issue with VK_ERROR_INCOMPATIBLE_DRIVER on MacOS, we need to enable “VK_KHR_portability_subset” extension.The we based on the above information included in VkDeviceCreateInfo struct, we create the logical device.The last step is to retrieve the queue handle we created.createSwapChain()When picking physical device, we already enable the swap chain support and query its detail, now it’s time for us to actually create swap chain.We will choose the properties of the swap chain including: surface format(pixel format, color space), presentation mode, swap extent, image count in a swap chain.After we choose the ideal value of the properties, we will create the swap chain object.I’ll keep the detail of the properties selection for now. It is better for me to get the bigger picture first.createImageViews()A VkImageView describes how to view an image. We will create VkImageView that match the image in the swap chain. That is to say we will create a VkImageView object for every image in the swap chain. In this simple demo, we simply treat the images as 2D textures.createRenderPass()Render pass object wrap the information for the framebuffer attachments. Including the number of color and depth buffers that will be used while rendering, number of samples for each buffer, etc. For this demo, a single color buffer attachment and a subpass were created for the render pass object." }, { "title": "Android Studio Giraffe + Kotlin + OpenCV 4.8.0", "url": "/posts/android-studio-giraffe-kotlin-opencv-4-8-0/", "categories": "", "tags": "", "date": "2023-09-07 13:22:00 -0500", "snippet": "After hours of attempting, I finally integrated OpenCV 4.8.0 to Android Studio Giraffe with the Kotlin project.I’ll skip the steps for downloading AS and OpenCV. Just remember to download the Android version of OpenCV and extract the zipped file to whatever place you want.Create a new “Empty Activity” projectI want to use Compose to develop the UI of the app. Currently, I only learned this from Android Studio’s official tutorial. So just create an “Empty Activity”.File→New→New Project→Empty Activity→NextRemember to change the “Build configuration Language” from default to “Groovy DSL(build.gradle)”Then click “Finish”Import OpenCV SDK as a new moduleFile→New→Import Module→Select the path to the place where you extracted the OpenCV SDK and change the module name to “OpenCV”Then click FinishNow, we will start our troubleshooting journey.Troubleshooting 1: Namespace not specifiedNamespace not specified. Specify a namespace in the module's build file. See https://d.android.com/r/tools/upgrade-assistant/set-namespace for information about setting the namespace.If you've specified the package attribute in the source AndroidManifest.xml, you can use the AGP Upgrade Assistant to migrate to the namespace value in the build file. Refer to https://d.android.com/r/tools/upgrade-assistant/agp-upgrade-assistant for general information about using the AGP Upgrade Assistant.Please check this link for more information related to this issue https://github.com/opencv/opencv/pull/23447To fix this issueChange the project view to “Project” where you can find the imported “opencv” module. Inside the “opencv” folder, open “build.gradle”, add “namespace ‘org.opencv’ to the file.Then sync the gradle by click Try Again on the top of this gradle file.Then this problem should be fix. BUILD SUCCESSFUL( which is obviously not yet).Add opencv as dependency as the appFile→Project Structure→Dependencies(tab)→app→+→3 Module Dependency→select opencv module as dependencies→OK→ApplyRun the app and lets fix some compile error of OpenCVTroubleshooting2: Execution failed for task ‘:opencv:compileDebugKotlin’Execution failed for task ':opencv:compileDebugKotlin'.&gt; 'compileDebugJavaWithJavac' task (current target is 1.8) and 'compileDebugKotlin' task (current target is 17) jvm target compatibility should be set to the same Java version. Consider using JVM toolchain: https://kotl.in/gradle/jvm/toolchainOpen build.gradle of opencv againAdd the following code to the end of the android{} sectionjava { toolchain { languageVersion = JavaLanguageVersion.of(8)\t }}buildFeatures { aidl true}And click Sync Now. BUILD SUCCESSFUL (not yet)Run the app again.Troubleshooting 3: Cannot find symbolerror: cannot find symbolimport org.opencv.BuildConfig;^symbol: class BuildConfiglocation: package org.opencvAdd one more buildFeaturebuildFeatures **{** \taidl true \tbuildConfig true**}**Now the app should be able to run.Congrats!Test OpenCV is ready to useimport opencv dependencies, add the OpenCVLoader.initDebug() in the onCreate fun.import org.opencv.android.OpenCVLoaderimport org.opencv.core.Coreclass MainActivity : ComponentActivity() { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) if(OpenCVLoader.initDebug()) { Log.d(\"OPENCV\",\"Opencv init\") println(\"opencv version: ${Core.VERSION}\") } else { Log.d(\"OPENCV\",\"Opencv init failed\") } setContent { OpenCVIntegrationFinalTheme { // A surface container using the 'background' color from the theme Surface( modifier = Modifier.fillMaxSize(), color = MaterialTheme.colorScheme.background ) { Greeting(\"Android\") } } } }}We can check the logcatIt’s working! Finally!Reference:https://medium.com/@ankitsachan/android-studio-kotlin-open-cv-16d75f8d9969https://forum.opencv.org/t/an-exercise-in-frustration/12964/3?u=ankit_sachan&amp;source=post_page—–16d75f8d9969——————————–" }, { "title": "Training on FLIC Dataset with Limited Memory in Tensorflow", "url": "/posts/training-on-flic-dataset-with-limited-memory-in-tensorflow/", "categories": "CV_Notes", "tags": "CV", "date": "2023-03-28 10:38:00 -0500", "snippet": "IntroductionI attempted to run the code from the joint_cnn_mrf, which is the first and only implementation of the paper “Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation.” However, I encountered several challenges due to the version difference between Tensorflow 1.x and Tensorflow 2.x, as well as memory limitations. The code requires loading the entire training and testing datasets into memory, which proved to be a challenge for my computer with only 32GB of RAM. I even tried to migrate it to Colab and upgraded to Colab Pro, but the data processing consumed more than 50GB of memory and used up the compute unit immediately. It seems the authors must have had a powerful computer even in 2018. As recommended by the authors, I have decided to implement the code from scratch.import tensorflow as tffrom tensorflow import kerasimport tensorflow_datasets as tfdsimport numpy as npimport cv2import mathfrom scipy.io import loadmatimport imageioDownload DataTo download file from google drive, share the file by link and set the access permission to everyone who knows the link. For example, my shared link is https://drive.google.com/file/d/16o7zXFl2PsFHXf9OVEc7OVJrQZ6ru1Mv/view?usp=share_link, copy the magic string ‘16o7zXFl2PsFHXf9OVEc7OVJrQZ6ru1Mv’!gdown 16o7zXFl2PsFHXf9OVEc7OVJrQZ6ru1MvDownloading...From: https://drive.google.com/uc?id=16o7zXFl2PsFHXf9OVEc7OVJrQZ6ru1MvTo: /content/FLIC.zip100% 300M/300M [00:02&lt;00:00, 138MB/s]Unzip the file!unzip '/content/FLIC.zip'Generate Ground Truth HeatmapsThe code is adapted from https://github.com/max-andr/joint-cnn-mrf#load the annotation data from the annotation file pathdata_FLIC = loadmat('/content/FLIC/examples.mat')data_FLIC = data_FLIC['examples'][0]#path to the image fileimages_dir = '/content/FLIC/images/'#define joint idsjoint_ids = ['lsho', 'lelb', 'lwri', 'rsho', 'relb', 'rwri', 'lhip', 'rhip', 'nose'] # , 'leye', 'reye',dict = {'lsho': 0, 'lelb': 1, 'lwri': 2, 'rsho': 3, 'relb': 4, 'rwri': 5, 'lhip': 6, 'lkne': 7, 'lank': 8, 'rhip': 9, 'rkne': 10, 'rank': 11, 'leye': 12, 'reye': 13, 'lear': 14, 'rear': 15, 'nose': 16, 'msho': 17, 'mhip': 18, 'mear': 19, 'mtorso': 20, 'mluarm': 21, 'mruarm': 22, 'mllarm': 23, 'mrlarm': 24, 'mluleg': 25, 'mruleg': 26, 'mllleg': 27, 'torso': 28}#get train and test data indexis_train = [data_FLIC[i][7][0, 0] for i in range(len(data_FLIC))]is_train = np.array(is_train)train_index = list(np.where(is_train == 1))[0]test_index = list(np.array(np.where(is_train == 0)))[0]print('# train indices:', len(train_index), ' # test indices:', len(test_index))# train indices: 3987 # test indices: 1016Define the Gaussian Kernel for generating the heatmaps.orig_h, orig_w = 480, 720coefs = np.array([[1, 2, 1]], dtype=np.float32) / 4 # maximizes performancekernel = coefs.T @ coefstemp = round((len(kernel) - 1) / 2)pad = 5 # use padding to avoid the exceeding of the boundaryCreate a list to store train filenames and also a list to store test filenames.In the meantime, generate groud-truth heatmap for train and test dataset.train_filenames = []test_filenames = []train_heatmaps = []test_heatmaps = []for filenames, hmaps, indices in zip([train_filenames,test_filenames], [train_heatmaps,test_heatmaps],[train_index, test_index]): for i in indices: filenames.append(data_FLIC[i][3][0]) flic_coords = data_FLIC[i][2] heatmaps = [] torso = (flic_coords[:, dict['lsho']] + flic_coords[:, dict['rhip']] + flic_coords[:, dict['rsho']] + flic_coords[:, dict['lhip']]) / 4 flic_coords[:, dict['torso']] = torso for joint in joint_ids + ['torso']: coords = np.copy(flic_coords[:, dict[joint]]) # there are some annotation that are outside of the image (annotators did a great job!) coords[0], coords[1] = max(min(coords[1], orig_h), 0), max(min(coords[0], orig_w), 0) coords /= 8 heat_map = np.zeros([60, 90], dtype=np.float32) heat_map = np.lib.pad(heat_map, ((pad, pad), (pad, pad)), 'constant', constant_values=0) coords = coords + pad h1_k, h2_k = int(coords[0] - temp), int(coords[0] + temp + 1) w1_k, w2_k = int(coords[1] - temp), int(coords[1] + temp + 1) heat_map[h1_k:h2_k, w1_k:w2_k] = kernel heat_map = heat_map[pad:pad + 60, pad:pad + 90] heatmaps.append(heat_map) hmap = np.stack(heatmaps, axis=2) hmaps.append(hmap)Check the length and shape of the data created.print(len(train_filenames))print(len(test_filenames))print(len(train_heatmaps))print(train_heatmaps[0].shape)print(len(test_heatmaps))print(test_heatmaps[0].shape)398710163987(60, 90, 10)1016(60, 90, 10)We can save the numpy file for latter usage. We don’t have to do the data process every time.np.save('x_train_filenames.npy',train_filenames)np.save('x_test_filenames.npy',test_filenames)np.save('y_train.npy',train_heatmaps)np.save('y_test.npy',test_heatmaps)Create Custom Data Generator for Training in TensorflowCreate a custom batch data generator, each time the generator will help us to load the images from the disk.class Flic_Generator(keras.utils.Sequence): def __init__(self, image_filenames, labels, batch_size) : self.image_filenames = image_filenames self.labels = labels self.batch_size = batch_size def __len__(self) : return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int) def __getitem__(self, idx) : batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size] batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size] #load the images and normalized it, in our case, the model takes the original size of the image as input #no need to resize x_array = np.array([imageio.v2.imread('/content/FLIC/images/' + str(file_name)) for file_name in batch_x])/255 y_array = np.array(batch_y) return x_array,y_arrayCreate train and test data generatorbatch_size = 32training_batch_generator = Flic_Generator(train_filenames,train_heatmaps,batch_size)test_batch_generator = Flic_Generator(test_filenames,test_heatmaps,batch_size)print(training_batch_generator.image_filenames[0])12-oclock-high-special-edition-00006361.jpgShow one imageimg = imageio.v2.imread('/content/FLIC/images/' + str(training_batch_generator.image_filenames[0]))import matplotlib.pyplot as pltplt.imshow(img)plt.axis('off')plt.show()Show the heatmapsheatmaps = training_batch_generator.labels[0]heatmaps.shape(60, 90, 10)#transpose the heatmaps arrayheatmaps = np.transpose(heatmaps,(2,0,1))heatmaps.shape(10, 60, 90)combined_heatmap = heatmaps[0]for i in range(1,10): combined_heatmap += heatmaps[i]plt.imshow(combined_heatmap)plt.axis('off')plt.show()Train and Test the modelWe will use fit_generator to train the model, and predict_generator to test the test dataset. Tensorflow will call the “getitem” in the background for each iteration.history = model.fit_generator(generator=training_batch_generator,epochs=30)predicted = model.predict_generator(test_batch_generator)" }, { "title": "CV Study Notes - Classification I", "url": "/posts/cv-study-notes-classification-i/", "categories": "CV_Notes", "tags": "CV", "date": "2023-02-20 18:08:00 -0600", "snippet": "Classification The definition of classification is: categorizing a given set of data into classesExample of classification(CAP5415 - Lecture 11 by L. Lazebnik)This post will focus on the intuition of the classification algorithms including k-nearest neighbor and SVM. It is very interesting to learn the motivation and intuition behind each algorithm. No code or math will be involved in this post.The Simplest Binary Classifier - Nearest NeighborIllustration of nearest neighbor(CAP5415 - Lecture 11 by L. Lazebnik) Given a new data point, calculate the distance between training samples, and take the class of the nearest sample.The method is very simple and intuitive, however, it has some obvious disadvantages: Not efficient: Has to calculate the distance for all the points Sensitive for out-flyerAn example of out-flyer, the black dot is the new example that needs to be classified, the blue dot is the nearest point, however, the black dot should belong to the red class.Solution?How about we calculate multiple neighbors and determine the class by the mean of the label?This lead to the K-nearest neighbor algorithm.K-nearest Neighbor Classifier(CAP5415 - Lecture 11 by L. Lazebnik)Instead of using the label of the nearest point, we can look around and consider the k-nearest neighbor. It’s simple and easy to implement but computationally expensive like the nearest neighbor classifier.Linear ClassifierMotivation The intuitive of classification is to find the boundary in the feature space of the samples.Classify data set using a line for 2D data or a plane for 3D data. Take 2D as an example, it is to find a linear function to separate the classes. However, how can we determine which line to use?More than one line can divide the data set(https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)Maximum Margin Linear ClassifierTo determine which line is the best separator, an intuitive idea is instead of drawing a zero-width line, we can draw the line with width, the wider the line is, the more precise the separation is.The lines with width.(https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)The maximum margin linear classifier is the linear classifier with the maximum margin. It is the simplest support vector machine, called linear support vector machine(LSVM). The line divided the two data sets with maximized width(margin). The training data points on the dashed line with the black circle are called support vectors. The distance from the support vector to the solid line is the margin. The model is determined by these support vectors.Support Vectors(https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)Issue: sometimes the data set is non-linearly separable, and the LSVM will perform poorly.Non-linearly separable(https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)Kernel SVMWe can project the non-linearly separable data into higher-dimensional space defined by some function(kernel), and thereby the data can fit with a linear classifier.Here we can map the data using Radius Basis Function (RBF kernel) centered on the middle point to map the data from the non-separable 2D dimension to the separable 3D dimension.Different kernel functions can be chosen to project the data to a higher dimension, but which kernel is best suitable for the data needed to be decided. Determining the center of the kernel is also difficult, but lucky for us, the built-in kernel trick process of SVM cover us.Softening MarginsSo far the data set in the example figures are all naturally separated. However, in the real case, the data points might be overlapping with each other, and there won’t a clear boundary between data points. Thus, another determining parameter for SVM is how soft the margin is which means allowing some of the points inside the margin if can get a better fit. If the margin is hard, the points can not get inside it. If it is soft, points can get inside the margin.Left is the hard margin, Right is the soft margin.References:https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.htmlhttps://www.crcv.ucf.edu/wp-content/uploads/2018/11/Lecture-11-Classification-I.pdf" }, { "title": "Image classification with CNN and Transfer Learning", "url": "/posts/image-classification-with-cnn-and-transfer-learning/", "categories": "CV_Notes", "tags": "CV", "date": "2023-01-31 12:38:00 -0600", "snippet": "COSC 6373: Computer Vision In-Class Assignment 2Image classification with Convolutional Neural Networks and Transfer LearningThis note is an assignment from COSC 6373 assignment.GoalUsing transfer learning technique on a pre-trained ResNet50 CNN model to perform classification for recognizing images of horses and camels. Tensorflow framework will be utilized to implement the task.IntroductionResNet50ResNet is a specific type of CNN means Residual Network which forms networks by stacking residual blocks. ResNet50 is a CNN with 50 layers(48 convolutional layers, one MaxPool layer, and one average pool layer). https://datagen.tech/guides/computer-vision/resnet-50/https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50ImageNetImageNet is a large scale hieararchical image database, contains more than 1.2 millions images of 1000 classeshttps://www.image-net.org/The ResNet50 model was pre-trained on ImageNet, so we can use the pre-trained weights.Transfer LearningTansfer leanring means transferring the knowledge of a pre-trained model to perform a new task. The pre-trained model is usually trained on a large scale dataset like the ImageNet for image-classification task. It is a generic model and the learned feature maps can be very useful. To train such a model from scratch requires lots of data, time, and resources. Thus, it is intuitive that using the feature extraction ability of the pretrained model to perform a new classification task on a small scale dataset.ExperimentImport the lib, in this task, tensorflow framework was usedimport matplotlib.pyplot as pltimport numpy as npimport osimport tensorflow as tfimport seaborn as snsfrom google.colab import filesData PreprocessingData DownloadLoad Dataset from google drive, and unzip the zip file to current contentfrom google.colab import drivedrive.mount('/content/drive')!unzip -q '/content/drive/MyDrive/Datasets/archive.zip'train_dir = '/content/train';test_dir = '/content/test';Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).Input pipeline using Tensorflow KerasReferences:https://www.tensorflow.org/tutorials/images/transfer_learninghttps://keras.io/guides/transfer_learning/Load dataset using tensorflow utils and create tf.data.Dataset object. In order to input the image to the ResNet50 model, the image size has to be 224 by 224. The resize process can be done during dataloader process via Keras.utils.image_dataset_from_directory.https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directoryThe batch size was set to 32, image size was resized to 224 by 224.The dataset do not contains validation data set, thus, 20% of data will be taken from the train data. For training and validation dataset, set shuffle = True will shuffle the data. For test data set shuffle = False to better evaluate the result.In the end, there was 10 batches for training data, 2 batches for validation, and 2 batches for testing.#set batch size to 32, but why 32?BATCH_SIZE = 32#the image size for ResNet50 model input should be 224*224, resize itIMG_SIZE = (224, 224)#for training and validation we set shuffle = Truedataset = tf.keras.utils.image_dataset_from_directory(train_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE)#for test we set shuffle = False to better evaluate the resulttest_dataset = tf.keras.utils.image_dataset_from_directory(test_dir, shuffle=False, batch_size=BATCH_SIZE, image_size=IMG_SIZE)# create a validation dataset from train_dataseval_batches = tf.data.experimental.cardinality(dataset)validation_dataset = dataset.take(val_batches // 5)train_dataset = dataset.skip(val_batches // 5)print('Number of dataset batches: %d' % tf.data.experimental.cardinality(dataset))print('Number of train batches: %d' % tf.data.experimental.cardinality(train_dataset))print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))Found 360 files belonging to 2 classes.Found 40 files belonging to 2 classes.Number of dataset batches: 12Number of train batches: 10Number of validation batches: 2Number of test batches: 2Show the 40 test_dataset images and labels.A question for this data loader part is why the original image contains only the object but no background, after the data loader, each image contains a background. And the backgroud is only in the bounding box area. Some image contains a large area of white space, will it affect the learn of the model?class_names = test_dataset.class_namesprint(class_names)plt.figure(figsize=(10, 10))for images, labels in test_dataset.take(-1): #print(len(images)) for i in range(len(images)): plt.subplot(5, 8, i + 1) plt.imshow(images[i].numpy().astype(\"uint8\")) plt.title(class_names[labels[i]]) plt.axis(\"off\") #print(images[i].shape)['camel', 'horse']&lt;ipython-input-4-b987f42f6524&gt;:8: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance. plt.subplot(5, 8, i + 1)Questions for the data loader The original image contains no background, but after loading through tf.keras.utils.image_dataset_from_directory, the background shows.After more research and communicate with classmate, the reason is the dataset actually contains background, but they use alpha channel to block the background, when loading to the tf.data.Dataset the image was converted to RGB, the alpha channel was removed, so the image is shown with the background.Take train/camel/10.png as an example.It shows that the image contains only the camel itself.Loaded using PILfrom PIL import Imagecamel = Image.open('/content/train/camel/10.png')plt.imshow(camel)plt.axis('off')plt.show()Convert to RGB, the background is shown.camel = Image.open('/content/train/camel/10.png').convert('RGB')plt.imshow(camel)plt.axis('off')plt.show() Why only the bounding box area contains background, the rest area of the image are all white?This is because, the dataset itself already pre-process the image to square image, it padding a rectangle image with white pixels to a square image. Will the white part of the image decrease the learning ability of the model and affect the performance?As for my classmate, he said, it depends on the model, ResNet is a very deep model, which can well extract local features, so in our case it would work well, however, for some shallow model, it would be bad.Configure the dataset for performancePrefetching is a transformation step, which overlaps the preprocessing and model execution of a training step to enable better performance to prevent I/O blocking by using a background thread and an internal buffer to prefetch loaded images from the disk before they are used.https://www.tensorflow.org/guide/data_performanceAUTOTUNE = tf.data.AUTOTUNEtrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)Use data augmentationThe camels and horses dataset only contains 400 images, so a data augmentation process may useful to train a more generic model and avoid overfitting. Typically, the data augmentation can be seen as sequential layers contains different transformation operation on the image, in the experiment, a random flip on the x axis of the image and a random rotation was applied.However, during the experiment, the result for the data augmentation seems not correct. Besides, the performance of the model without the data augmentation layers was slightly higher. More detail results will be shown in the end of this report.data_augmentation = tf.keras.Sequential([ tf.keras.layers.RandomFlip('horizontal'), tf.keras.layers.RandomRotation(0.2),])for image, _ in train_dataset.take(1): plt.figure(figsize=(10, 10)) first_image = image[0] for i in range(9): ax = plt.subplot(3, 3, i + 1) augmented_image = data_augmentation(tf.expand_dims(first_image, 0)) plt.imshow(augmented_image[0] / 255) plt.axis('off')Question for Data AugmentationWhy the result seems not correct like the one in the tutorial?https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotationPoints outside the boundaries of the input are filled according to the given mode (one of {“constant”, “reflect”, “wrap”, “nearest”}).reflect: (d c b a | a b c d | d c b a) The input is extended by reflecting about the edge of the last pixel.It seems like the empty part after random rotation is filled by reflecting abou the edge of the last pixel. The image contains white space, that’s why it looks weried.Rescale pixel valueA important step for transfer learning is the input should be processed to meet the expectation of the base model. Each Keras Application expects a specific kind of input preprocessing. For the preprocess_input of ResNet50 will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.At the first trail of the experiment, the preprocess_input was not included, and the result was slightly worse than the model with the preprocess_input. More results will be shown in the end of the report.preprocess_input = tf.keras.applications.resnet50.preprocess_inputCreate Base ModelA base model from the ResNet50 model was created which pre-trained on the ImageNet dataset. This base of knowledge will be benifical to classify camels and horses from our specific dataset.According to the common practice, the features of “bottleneck layer” which is the last layer before the flatten operation are more generality than the top classification layer of the base model. Thus, to do transfer learning, the top classification layer of the base model was removed and the base model was used as a feature extractor.# Create the base model from ResNet50 using pre-trained weight on ImageNet, exclude the top layerIMG_SHAPE = IMG_SIZE + (3,)base_model = tf.keras.applications.ResNet50(weights = 'imagenet', include_top = False, input_shape = IMG_SHAPE)The base model which was served as a feature extractor in our case, converts each 2242243 image into a 7x7x2048 block of features.image_batch, label_batch = next(iter(train_dataset))feature_batch = base_model(image_batch)print('feature_batch_shape: ', feature_batch.shape)feature_batch_shape: (32, 7, 7, 2048)Feature ExtractionIn this step, the base model will be freezed and to use as a feature extractor, which means the base model is not trainable, the function for the base model is to extract generic features for the later specific classification task. In order to do the classification task, a classifiction layer will be added on top of base model. And will train the top-level classifier on our dataset.Freeze the Base Model#Freeze the base modelbase_model.trainable = False#base model architecture#noticed that the trainable parameters here was 0 because we set trainable = false, basically freeze the modelbase_model.summary()Model: \"resnet50\"__________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 224, 224, 3 0 [] )] conv1_pad (ZeroPadding2D) (None, 230, 230, 3) 0 ['input_1[0][0]'] conv1_conv (Conv2D) (None, 112, 112, 64 9472 ['conv1_pad[0][0]'] ) conv1_bn (BatchNormalization) (None, 112, 112, 64 256 ['conv1_conv[0][0]'] ) conv1_relu (Activation) (None, 112, 112, 64 0 ['conv1_bn[0][0]'] ) pool1_pad (ZeroPadding2D) (None, 114, 114, 64 0 ['conv1_relu[0][0]'] ) pool1_pool (MaxPooling2D) (None, 56, 56, 64) 0 ['pool1_pad[0][0]'] conv2_block1_1_conv (Conv2D) (None, 56, 56, 64) 4160 ['pool1_pool[0][0]'] conv2_block1_1_bn (BatchNormal (None, 56, 56, 64) 256 ['conv2_block1_1_conv[0][0]'] ization) conv2_block1_1_relu (Activatio (None, 56, 56, 64) 0 ['conv2_block1_1_bn[0][0]'] n) conv2_block1_2_conv (Conv2D) (None, 56, 56, 64) 36928 ['conv2_block1_1_relu[0][0]'] conv2_block1_2_bn (BatchNormal (None, 56, 56, 64) 256 ['conv2_block1_2_conv[0][0]'] ization) conv2_block1_2_relu (Activatio (None, 56, 56, 64) 0 ['conv2_block1_2_bn[0][0]'] n) conv2_block1_0_conv (Conv2D) (None, 56, 56, 256) 16640 ['pool1_pool[0][0]'] conv2_block1_3_conv (Conv2D) (None, 56, 56, 256) 16640 ['conv2_block1_2_relu[0][0]'] conv2_block1_0_bn (BatchNormal (None, 56, 56, 256) 1024 ['conv2_block1_0_conv[0][0]'] ization) conv2_block1_3_bn (BatchNormal (None, 56, 56, 256) 1024 ['conv2_block1_3_conv[0][0]'] ization) conv2_block1_add (Add) (None, 56, 56, 256) 0 ['conv2_block1_0_bn[0][0]', 'conv2_block1_3_bn[0][0]'] conv2_block1_out (Activation) (None, 56, 56, 256) 0 ['conv2_block1_add[0][0]'] conv2_block2_1_conv (Conv2D) (None, 56, 56, 64) 16448 ['conv2_block1_out[0][0]'] conv2_block2_1_bn (BatchNormal (None, 56, 56, 64) 256 ['conv2_block2_1_conv[0][0]'] ization) conv2_block2_1_relu (Activatio (None, 56, 56, 64) 0 ['conv2_block2_1_bn[0][0]'] n) conv2_block2_2_conv (Conv2D) (None, 56, 56, 64) 36928 ['conv2_block2_1_relu[0][0]'] conv2_block2_2_bn (BatchNormal (None, 56, 56, 64) 256 ['conv2_block2_2_conv[0][0]'] ization) conv2_block2_2_relu (Activatio (None, 56, 56, 64) 0 ['conv2_block2_2_bn[0][0]'] n) conv2_block2_3_conv (Conv2D) (None, 56, 56, 256) 16640 ['conv2_block2_2_relu[0][0]'] conv2_block2_3_bn (BatchNormal (None, 56, 56, 256) 1024 ['conv2_block2_3_conv[0][0]'] ization) conv2_block2_add (Add) (None, 56, 56, 256) 0 ['conv2_block1_out[0][0]', 'conv2_block2_3_bn[0][0]'] conv2_block2_out (Activation) (None, 56, 56, 256) 0 ['conv2_block2_add[0][0]'] conv2_block3_1_conv (Conv2D) (None, 56, 56, 64) 16448 ['conv2_block2_out[0][0]'] conv2_block3_1_bn (BatchNormal (None, 56, 56, 64) 256 ['conv2_block3_1_conv[0][0]'] ization) conv2_block3_1_relu (Activatio (None, 56, 56, 64) 0 ['conv2_block3_1_bn[0][0]'] n) conv2_block3_2_conv (Conv2D) (None, 56, 56, 64) 36928 ['conv2_block3_1_relu[0][0]'] conv2_block3_2_bn (BatchNormal (None, 56, 56, 64) 256 ['conv2_block3_2_conv[0][0]'] ization) conv2_block3_2_relu (Activatio (None, 56, 56, 64) 0 ['conv2_block3_2_bn[0][0]'] n) conv2_block3_3_conv (Conv2D) (None, 56, 56, 256) 16640 ['conv2_block3_2_relu[0][0]'] conv2_block3_3_bn (BatchNormal (None, 56, 56, 256) 1024 ['conv2_block3_3_conv[0][0]'] ization) conv2_block3_add (Add) (None, 56, 56, 256) 0 ['conv2_block2_out[0][0]', 'conv2_block3_3_bn[0][0]'] conv2_block3_out (Activation) (None, 56, 56, 256) 0 ['conv2_block3_add[0][0]'] conv3_block1_1_conv (Conv2D) (None, 28, 28, 128) 32896 ['conv2_block3_out[0][0]'] conv3_block1_1_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block1_1_conv[0][0]'] ization) conv3_block1_1_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block1_1_bn[0][0]'] n) conv3_block1_2_conv (Conv2D) (None, 28, 28, 128) 147584 ['conv3_block1_1_relu[0][0]'] conv3_block1_2_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block1_2_conv[0][0]'] ization) conv3_block1_2_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block1_2_bn[0][0]'] n) conv3_block1_0_conv (Conv2D) (None, 28, 28, 512) 131584 ['conv2_block3_out[0][0]'] conv3_block1_3_conv (Conv2D) (None, 28, 28, 512) 66048 ['conv3_block1_2_relu[0][0]'] conv3_block1_0_bn (BatchNormal (None, 28, 28, 512) 2048 ['conv3_block1_0_conv[0][0]'] ization) conv3_block1_3_bn (BatchNormal (None, 28, 28, 512) 2048 ['conv3_block1_3_conv[0][0]'] ization) conv3_block1_add (Add) (None, 28, 28, 512) 0 ['conv3_block1_0_bn[0][0]', 'conv3_block1_3_bn[0][0]'] conv3_block1_out (Activation) (None, 28, 28, 512) 0 ['conv3_block1_add[0][0]'] conv3_block2_1_conv (Conv2D) (None, 28, 28, 128) 65664 ['conv3_block1_out[0][0]'] conv3_block2_1_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block2_1_conv[0][0]'] ization) conv3_block2_1_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block2_1_bn[0][0]'] n) conv3_block2_2_conv (Conv2D) (None, 28, 28, 128) 147584 ['conv3_block2_1_relu[0][0]'] conv3_block2_2_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block2_2_conv[0][0]'] ization) conv3_block2_2_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block2_2_bn[0][0]'] n) conv3_block2_3_conv (Conv2D) (None, 28, 28, 512) 66048 ['conv3_block2_2_relu[0][0]'] conv3_block2_3_bn (BatchNormal (None, 28, 28, 512) 2048 ['conv3_block2_3_conv[0][0]'] ization) conv3_block2_add (Add) (None, 28, 28, 512) 0 ['conv3_block1_out[0][0]', 'conv3_block2_3_bn[0][0]'] conv3_block2_out (Activation) (None, 28, 28, 512) 0 ['conv3_block2_add[0][0]'] conv3_block3_1_conv (Conv2D) (None, 28, 28, 128) 65664 ['conv3_block2_out[0][0]'] conv3_block3_1_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block3_1_conv[0][0]'] ization) conv3_block3_1_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block3_1_bn[0][0]'] n) conv3_block3_2_conv (Conv2D) (None, 28, 28, 128) 147584 ['conv3_block3_1_relu[0][0]'] conv3_block3_2_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block3_2_conv[0][0]'] ization) conv3_block3_2_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block3_2_bn[0][0]'] n) conv3_block3_3_conv (Conv2D) (None, 28, 28, 512) 66048 ['conv3_block3_2_relu[0][0]'] conv3_block3_3_bn (BatchNormal (None, 28, 28, 512) 2048 ['conv3_block3_3_conv[0][0]'] ization) conv3_block3_add (Add) (None, 28, 28, 512) 0 ['conv3_block2_out[0][0]', 'conv3_block3_3_bn[0][0]'] conv3_block3_out (Activation) (None, 28, 28, 512) 0 ['conv3_block3_add[0][0]'] conv3_block4_1_conv (Conv2D) (None, 28, 28, 128) 65664 ['conv3_block3_out[0][0]'] conv3_block4_1_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block4_1_conv[0][0]'] ization) conv3_block4_1_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block4_1_bn[0][0]'] n) conv3_block4_2_conv (Conv2D) (None, 28, 28, 128) 147584 ['conv3_block4_1_relu[0][0]'] conv3_block4_2_bn (BatchNormal (None, 28, 28, 128) 512 ['conv3_block4_2_conv[0][0]'] ization) conv3_block4_2_relu (Activatio (None, 28, 28, 128) 0 ['conv3_block4_2_bn[0][0]'] n) conv3_block4_3_conv (Conv2D) (None, 28, 28, 512) 66048 ['conv3_block4_2_relu[0][0]'] conv3_block4_3_bn (BatchNormal (None, 28, 28, 512) 2048 ['conv3_block4_3_conv[0][0]'] ization) conv3_block4_add (Add) (None, 28, 28, 512) 0 ['conv3_block3_out[0][0]', 'conv3_block4_3_bn[0][0]'] conv3_block4_out (Activation) (None, 28, 28, 512) 0 ['conv3_block4_add[0][0]'] conv4_block1_1_conv (Conv2D) (None, 14, 14, 256) 131328 ['conv3_block4_out[0][0]'] conv4_block1_1_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block1_1_conv[0][0]'] ization) conv4_block1_1_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block1_1_bn[0][0]'] n) conv4_block1_2_conv (Conv2D) (None, 14, 14, 256) 590080 ['conv4_block1_1_relu[0][0]'] conv4_block1_2_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block1_2_conv[0][0]'] ization) conv4_block1_2_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block1_2_bn[0][0]'] n) conv4_block1_0_conv (Conv2D) (None, 14, 14, 1024 525312 ['conv3_block4_out[0][0]'] ) conv4_block1_3_conv (Conv2D) (None, 14, 14, 1024 263168 ['conv4_block1_2_relu[0][0]'] ) conv4_block1_0_bn (BatchNormal (None, 14, 14, 1024 4096 ['conv4_block1_0_conv[0][0]'] ization) ) conv4_block1_3_bn (BatchNormal (None, 14, 14, 1024 4096 ['conv4_block1_3_conv[0][0]'] ization) ) conv4_block1_add (Add) (None, 14, 14, 1024 0 ['conv4_block1_0_bn[0][0]', ) 'conv4_block1_3_bn[0][0]'] conv4_block1_out (Activation) (None, 14, 14, 1024 0 ['conv4_block1_add[0][0]'] ) conv4_block2_1_conv (Conv2D) (None, 14, 14, 256) 262400 ['conv4_block1_out[0][0]'] conv4_block2_1_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block2_1_conv[0][0]'] ization) conv4_block2_1_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block2_1_bn[0][0]'] n) conv4_block2_2_conv (Conv2D) (None, 14, 14, 256) 590080 ['conv4_block2_1_relu[0][0]'] conv4_block2_2_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block2_2_conv[0][0]'] ization) conv4_block2_2_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block2_2_bn[0][0]'] n) conv4_block2_3_conv (Conv2D) (None, 14, 14, 1024 263168 ['conv4_block2_2_relu[0][0]'] ) conv4_block2_3_bn (BatchNormal (None, 14, 14, 1024 4096 ['conv4_block2_3_conv[0][0]'] ization) ) conv4_block2_add (Add) (None, 14, 14, 1024 0 ['conv4_block1_out[0][0]', ) 'conv4_block2_3_bn[0][0]'] conv4_block2_out (Activation) (None, 14, 14, 1024 0 ['conv4_block2_add[0][0]'] ) conv4_block3_1_conv (Conv2D) (None, 14, 14, 256) 262400 ['conv4_block2_out[0][0]'] conv4_block3_1_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block3_1_conv[0][0]'] ization) conv4_block3_1_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block3_1_bn[0][0]'] n) conv4_block3_2_conv (Conv2D) (None, 14, 14, 256) 590080 ['conv4_block3_1_relu[0][0]'] conv4_block3_2_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block3_2_conv[0][0]'] ization) conv4_block3_2_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block3_2_bn[0][0]'] n) conv4_block3_3_conv (Conv2D) (None, 14, 14, 1024 263168 ['conv4_block3_2_relu[0][0]'] ) conv4_block3_3_bn (BatchNormal (None, 14, 14, 1024 4096 ['conv4_block3_3_conv[0][0]'] ization) ) conv4_block3_add (Add) (None, 14, 14, 1024 0 ['conv4_block2_out[0][0]', ) 'conv4_block3_3_bn[0][0]'] conv4_block3_out (Activation) (None, 14, 14, 1024 0 ['conv4_block3_add[0][0]'] ) conv4_block4_1_conv (Conv2D) (None, 14, 14, 256) 262400 ['conv4_block3_out[0][0]'] conv4_block4_1_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block4_1_conv[0][0]'] ization) conv4_block4_1_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block4_1_bn[0][0]'] n) conv4_block4_2_conv (Conv2D) (None, 14, 14, 256) 590080 ['conv4_block4_1_relu[0][0]'] conv4_block4_2_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block4_2_conv[0][0]'] ization) conv4_block4_2_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block4_2_bn[0][0]'] n) conv4_block4_3_conv (Conv2D) (None, 14, 14, 1024 263168 ['conv4_block4_2_relu[0][0]'] ) conv4_block4_3_bn (BatchNormal (None, 14, 14, 1024 4096 ['conv4_block4_3_conv[0][0]'] ization) ) conv4_block4_add (Add) (None, 14, 14, 1024 0 ['conv4_block3_out[0][0]', ) 'conv4_block4_3_bn[0][0]'] conv4_block4_out (Activation) (None, 14, 14, 1024 0 ['conv4_block4_add[0][0]'] ) conv4_block5_1_conv (Conv2D) (None, 14, 14, 256) 262400 ['conv4_block4_out[0][0]'] conv4_block5_1_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block5_1_conv[0][0]'] ization) conv4_block5_1_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block5_1_bn[0][0]'] n) conv4_block5_2_conv (Conv2D) (None, 14, 14, 256) 590080 ['conv4_block5_1_relu[0][0]'] conv4_block5_2_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block5_2_conv[0][0]'] ization) conv4_block5_2_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block5_2_bn[0][0]'] n) conv4_block5_3_conv (Conv2D) (None, 14, 14, 1024 263168 ['conv4_block5_2_relu[0][0]'] ) conv4_block5_3_bn (BatchNormal (None, 14, 14, 1024 4096 ['conv4_block5_3_conv[0][0]'] ization) ) conv4_block5_add (Add) (None, 14, 14, 1024 0 ['conv4_block4_out[0][0]', ) 'conv4_block5_3_bn[0][0]'] conv4_block5_out (Activation) (None, 14, 14, 1024 0 ['conv4_block5_add[0][0]'] ) conv4_block6_1_conv (Conv2D) (None, 14, 14, 256) 262400 ['conv4_block5_out[0][0]'] conv4_block6_1_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block6_1_conv[0][0]'] ization) conv4_block6_1_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block6_1_bn[0][0]'] n) conv4_block6_2_conv (Conv2D) (None, 14, 14, 256) 590080 ['conv4_block6_1_relu[0][0]'] conv4_block6_2_bn (BatchNormal (None, 14, 14, 256) 1024 ['conv4_block6_2_conv[0][0]'] ization) conv4_block6_2_relu (Activatio (None, 14, 14, 256) 0 ['conv4_block6_2_bn[0][0]'] n) conv4_block6_3_conv (Conv2D) (None, 14, 14, 1024 263168 ['conv4_block6_2_relu[0][0]'] ) conv4_block6_3_bn (BatchNormal (None, 14, 14, 1024 4096 ['conv4_block6_3_conv[0][0]'] ization) ) conv4_block6_add (Add) (None, 14, 14, 1024 0 ['conv4_block5_out[0][0]', ) 'conv4_block6_3_bn[0][0]'] conv4_block6_out (Activation) (None, 14, 14, 1024 0 ['conv4_block6_add[0][0]'] ) conv5_block1_1_conv (Conv2D) (None, 7, 7, 512) 524800 ['conv4_block6_out[0][0]'] conv5_block1_1_bn (BatchNormal (None, 7, 7, 512) 2048 ['conv5_block1_1_conv[0][0]'] ization) conv5_block1_1_relu (Activatio (None, 7, 7, 512) 0 ['conv5_block1_1_bn[0][0]'] n) conv5_block1_2_conv (Conv2D) (None, 7, 7, 512) 2359808 ['conv5_block1_1_relu[0][0]'] conv5_block1_2_bn (BatchNormal (None, 7, 7, 512) 2048 ['conv5_block1_2_conv[0][0]'] ization) conv5_block1_2_relu (Activatio (None, 7, 7, 512) 0 ['conv5_block1_2_bn[0][0]'] n) conv5_block1_0_conv (Conv2D) (None, 7, 7, 2048) 2099200 ['conv4_block6_out[0][0]'] conv5_block1_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 ['conv5_block1_2_relu[0][0]'] conv5_block1_0_bn (BatchNormal (None, 7, 7, 2048) 8192 ['conv5_block1_0_conv[0][0]'] ization) conv5_block1_3_bn (BatchNormal (None, 7, 7, 2048) 8192 ['conv5_block1_3_conv[0][0]'] ization) conv5_block1_add (Add) (None, 7, 7, 2048) 0 ['conv5_block1_0_bn[0][0]', 'conv5_block1_3_bn[0][0]'] conv5_block1_out (Activation) (None, 7, 7, 2048) 0 ['conv5_block1_add[0][0]'] conv5_block2_1_conv (Conv2D) (None, 7, 7, 512) 1049088 ['conv5_block1_out[0][0]'] conv5_block2_1_bn (BatchNormal (None, 7, 7, 512) 2048 ['conv5_block2_1_conv[0][0]'] ization) conv5_block2_1_relu (Activatio (None, 7, 7, 512) 0 ['conv5_block2_1_bn[0][0]'] n) conv5_block2_2_conv (Conv2D) (None, 7, 7, 512) 2359808 ['conv5_block2_1_relu[0][0]'] conv5_block2_2_bn (BatchNormal (None, 7, 7, 512) 2048 ['conv5_block2_2_conv[0][0]'] ization) conv5_block2_2_relu (Activatio (None, 7, 7, 512) 0 ['conv5_block2_2_bn[0][0]'] n) conv5_block2_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 ['conv5_block2_2_relu[0][0]'] conv5_block2_3_bn (BatchNormal (None, 7, 7, 2048) 8192 ['conv5_block2_3_conv[0][0]'] ization) conv5_block2_add (Add) (None, 7, 7, 2048) 0 ['conv5_block1_out[0][0]', 'conv5_block2_3_bn[0][0]'] conv5_block2_out (Activation) (None, 7, 7, 2048) 0 ['conv5_block2_add[0][0]'] conv5_block3_1_conv (Conv2D) (None, 7, 7, 512) 1049088 ['conv5_block2_out[0][0]'] conv5_block3_1_bn (BatchNormal (None, 7, 7, 512) 2048 ['conv5_block3_1_conv[0][0]'] ization) conv5_block3_1_relu (Activatio (None, 7, 7, 512) 0 ['conv5_block3_1_bn[0][0]'] n) conv5_block3_2_conv (Conv2D) (None, 7, 7, 512) 2359808 ['conv5_block3_1_relu[0][0]'] conv5_block3_2_bn (BatchNormal (None, 7, 7, 512) 2048 ['conv5_block3_2_conv[0][0]'] ization) conv5_block3_2_relu (Activatio (None, 7, 7, 512) 0 ['conv5_block3_2_bn[0][0]'] n) conv5_block3_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 ['conv5_block3_2_relu[0][0]'] conv5_block3_3_bn (BatchNormal (None, 7, 7, 2048) 8192 ['conv5_block3_3_conv[0][0]'] ization) conv5_block3_add (Add) (None, 7, 7, 2048) 0 ['conv5_block2_out[0][0]', 'conv5_block3_3_bn[0][0]'] conv5_block3_out (Activation) (None, 7, 7, 2048) 0 ['conv5_block3_add[0][0]'] ==================================================================================================Total params: 23,587,712Trainable params: 0Non-trainable params: 23,587,712__________________________________________________________________________________________________Add classification layerIn order to perform the prediction task, the 772048 blocks of features has to be convert to a single prediction. This can be done by first add a average pooling 2D layer to convert the 772048 features to a single vector with 2048 elements, then apply a dense layer to convert these features into a single prediction per image. The raw prediction value will be the output of the model, the positive number predict class 1, negative number predict class 0.#add classification layer on top of the base model to train our datasetglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()print(\"feature extracted: \", feature_batch.shape)feature_batch_average = global_average_layer(feature_batch)print(\"feature after average layer: \", feature_batch_average.shape)prediction_layer = tf.keras.layers.Dense(1)prediction_batch = prediction_layer(feature_batch_average)print(\"feature after dense: \", prediction_batch.shape)feature extracted: (32, 7, 7, 2048)feature after average layer: (32, 2048)feature after dense: (32, 1)Connect the modelBuild a new model for the classification task by connecting the preprocessing layer, (the data augmentaion layer as an option), the base_model, and classficiation layers.In order to perform fine-tuning in the latter step, since our base model contains BatchNormalization layers, it is important to freeze the layer by setting training = False, during the fine-tuning step to aviod non-trainable weights to destory the knowledge of the model.#connect the layers with base model#input-&gt;data augmentation layer(optinonal)-&gt;preprocess layer-&gt;base model(without top)#-&gt;average pooling layer-&gt;Dense 1D layer(prediction)-&gt;outputinputs = tf.keras.Input(shape=IMG_SHAPE)x = inputs#here, we didn't use the data augmentation step#x = data_augmentation(inputs)x = preprocess_input(x)#The base model contains BatchNormalization layer, needed to be freezed in the trainning processx = base_model(x, training=False)x = global_average_layer(x)x = tf.keras.layers.Dropout(0.2)(x)outputs = prediction_layer(x)model = tf.keras.Model(inputs, outputs)Compile the ModelBefore training the model, the model has to be compiled. The optimizer we used is Adam, with learning rate equals to 0.0001.The loss function we used is binary crossentropy because we are dealing with two classes classification, with from_logits=True, because the output of the model is linear.#compile the modelbase_learning_rate = 0.0001model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate), loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])model.summary()Model: \"model\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 224, 224, 3)] 0 tf.__operators__.getitem (S (None, 224, 224, 3) 0 licingOpLambda) tf.nn.bias_add (TFOpLambda) (None, 224, 224, 3) 0 resnet50 (Functional) (None, 7, 7, 2048) 23587712 global_average_pooling2d (G (None, 2048) 0 lobalAveragePooling2D) dropout (Dropout) (None, 2048) 0 dense (Dense) (None, 1) 2049 =================================================================Total params: 23,589,761Trainable params: 2,049Non-trainable params: 23,587,712_________________________________________________________________Train the ModelThe model will reach approximately 90% accuracy on validation dataset after 30 epoches.The model’s train loss is 0.2691 and train accuracy is 0.9122. For the validation loss is 0.2502 and validation accuracy is 0.9062#train model until get 80% accuracy for validation dataset initial_epochs = 30loss0, accuracy0 = model.evaluate(validation_dataset)print(\"initial loss: {:.2f}\".format(loss0))print(\"initial accuracy: {:.2f}\".format(accuracy0))2/2 [==============================] - 3s 93ms/step - loss: 0.6686 - accuracy: 0.5312initial loss: 0.67initial accuracy: 0.53history = model.fit(train_dataset, epochs=initial_epochs, validation_data=validation_dataset)Epoch 1/3010/10 [==============================] - 5s 208ms/step - loss: 0.6526 - accuracy: 0.6081 - val_loss: 0.6622 - val_accuracy: 0.5625Epoch 2/3010/10 [==============================] - 2s 164ms/step - loss: 0.6865 - accuracy: 0.5777 - val_loss: 0.5727 - val_accuracy: 0.5938Epoch 3/3010/10 [==============================] - 2s 163ms/step - loss: 0.6350 - accuracy: 0.6115 - val_loss: 0.4946 - val_accuracy: 0.7656Epoch 4/3010/10 [==============================] - 2s 165ms/step - loss: 0.5700 - accuracy: 0.6757 - val_loss: 0.5301 - val_accuracy: 0.7344Epoch 5/3010/10 [==============================] - 2s 164ms/step - loss: 0.5833 - accuracy: 0.6689 - val_loss: 0.5904 - val_accuracy: 0.6094Epoch 6/3010/10 [==============================] - 2s 164ms/step - loss: 0.5633 - accuracy: 0.6655 - val_loss: 0.5028 - val_accuracy: 0.6719Epoch 7/3010/10 [==============================] - 2s 163ms/step - loss: 0.5220 - accuracy: 0.6993 - val_loss: 0.4518 - val_accuracy: 0.7812Epoch 8/3010/10 [==============================] - 2s 164ms/step - loss: 0.4983 - accuracy: 0.7095 - val_loss: 0.4269 - val_accuracy: 0.7812Epoch 9/3010/10 [==============================] - 2s 166ms/step - loss: 0.4650 - accuracy: 0.7466 - val_loss: 0.4626 - val_accuracy: 0.7031Epoch 10/3010/10 [==============================] - 2s 165ms/step - loss: 0.4773 - accuracy: 0.7399 - val_loss: 0.4236 - val_accuracy: 0.7812Epoch 11/3010/10 [==============================] - 3s 229ms/step - loss: 0.4382 - accuracy: 0.7770 - val_loss: 0.3895 - val_accuracy: 0.8125Epoch 12/3010/10 [==============================] - 2s 167ms/step - loss: 0.4280 - accuracy: 0.7703 - val_loss: 0.4166 - val_accuracy: 0.8594Epoch 13/3010/10 [==============================] - 2s 165ms/step - loss: 0.3893 - accuracy: 0.8176 - val_loss: 0.3135 - val_accuracy: 0.9219Epoch 14/3010/10 [==============================] - 2s 162ms/step - loss: 0.4067 - accuracy: 0.7872 - val_loss: 0.3374 - val_accuracy: 0.8750Epoch 15/3010/10 [==============================] - 2s 164ms/step - loss: 0.3876 - accuracy: 0.8041 - val_loss: 0.3294 - val_accuracy: 0.9062Epoch 16/3010/10 [==============================] - 2s 166ms/step - loss: 0.3630 - accuracy: 0.8378 - val_loss: 0.3480 - val_accuracy: 0.7969Epoch 17/3010/10 [==============================] - 2s 164ms/step - loss: 0.3430 - accuracy: 0.8615 - val_loss: 0.2949 - val_accuracy: 0.9375Epoch 18/3010/10 [==============================] - 3s 180ms/step - loss: 0.3618 - accuracy: 0.8209 - val_loss: 0.2876 - val_accuracy: 0.9531Epoch 19/3010/10 [==============================] - 3s 162ms/step - loss: 0.3300 - accuracy: 0.8480 - val_loss: 0.2808 - val_accuracy: 0.8906Epoch 20/3010/10 [==============================] - 2s 165ms/step - loss: 0.3163 - accuracy: 0.8716 - val_loss: 0.3115 - val_accuracy: 0.8906Epoch 21/3010/10 [==============================] - 2s 163ms/step - loss: 0.3254 - accuracy: 0.8851 - val_loss: 0.2884 - val_accuracy: 0.9375Epoch 22/3010/10 [==============================] - 2s 164ms/step - loss: 0.3067 - accuracy: 0.8919 - val_loss: 0.2604 - val_accuracy: 0.9219Epoch 23/3010/10 [==============================] - 2s 166ms/step - loss: 0.3052 - accuracy: 0.8986 - val_loss: 0.2277 - val_accuracy: 0.9688Epoch 24/3010/10 [==============================] - 2s 168ms/step - loss: 0.2866 - accuracy: 0.8716 - val_loss: 0.2730 - val_accuracy: 0.9062Epoch 25/3010/10 [==============================] - 2s 165ms/step - loss: 0.2824 - accuracy: 0.9054 - val_loss: 0.2647 - val_accuracy: 0.9219Epoch 26/3010/10 [==============================] - 2s 164ms/step - loss: 0.2776 - accuracy: 0.8818 - val_loss: 0.2575 - val_accuracy: 0.9062Epoch 27/3010/10 [==============================] - 2s 167ms/step - loss: 0.2533 - accuracy: 0.9088 - val_loss: 0.2338 - val_accuracy: 0.8906Epoch 28/3010/10 [==============================] - 2s 164ms/step - loss: 0.2690 - accuracy: 0.8953 - val_loss: 0.2692 - val_accuracy: 0.8750Epoch 29/3010/10 [==============================] - 2s 163ms/step - loss: 0.2684 - accuracy: 0.8953 - val_loss: 0.1831 - val_accuracy: 0.9531Epoch 30/3010/10 [==============================] - 2s 165ms/step - loss: 0.2564 - accuracy: 0.9020 - val_loss: 0.2330 - val_accuracy: 0.9062Learning CurveThe learning curves of the training and validation accuracy/loss when using the ResNet50 base model as a fixed feature extractor.The learning Curve is not as smooth as the tutorial, but still can observe the trend of the accuracy and loss. For the accuracy after each epoch the accuracy will be higher, and for the loss after each epoch the loss will be lower.As for the difference between train dataset and validation dataset, the validatation dataset shows higher accuracy and lower loss, this is because the tf.keras.layers.BatchNormalization and tf.keras.layers.Dropout affect the accuracy during training. They are turned off when calculating validation loss.#draw the learning curveacc = history.history['accuracy']val_acc = history.history['val_accuracy']loss = history.history['loss']val_loss = history.history['val_loss']plt.figure(figsize=(8, 8))plt.subplot(2, 1, 1)plt.plot(acc, label='Training Accuracy')plt.plot(val_acc, label='Validation Accuracy')plt.legend(loc='lower right')plt.ylabel('Accuracy')plt.ylim([min(plt.ylim()),1])plt.title('Training and Validation Accuracy')plt.subplot(2, 1, 2)plt.plot(loss, label='Training Loss')plt.plot(val_loss, label='Validation Loss')plt.legend(loc='upper right')plt.ylabel('Cross Entropy')plt.ylim([0,1.0])plt.title('Training and Validation Loss')plt.xlabel('epoch')plt.savefig('learning_curve_feature_extraction.png')plt.show()Fine TuningDuring the feature extraction step, the base model was served as a feature extractor, no parameters or weights were updated during the trianing process. It is also practical to do fine-tuning on the base model which can increase the performance even further.The fine-tune can be done by unfreeze a small number of top layers of the base model and retrain the model using the dataset. This can force the weights to be adapted from the generic feature maps pre-trained on the large image dataset to the more specific feature maps associated with the camels and horses.The reason for only fine-tune on the top layers is the features in these layers are more specific to the dataset than the lower layers.Important notes: The fine-tune can only be done after trained the new classification layer with the base model freezed. Otherwise, to train a classification layer from random weights will cause the gradient magnitue increased and will let the base model forget what it learned. Fine-tuning should be done with a lower learning rate to avoid overfittingUnfreeze Top layer#unfreeze the base modelbase_model.trainable = True#print the number of layers in the base modelprint(\"base model layers: \", len(base_model.layers))# select the layers to be fine-tunefine_tune_at = 100# Freeze other layersfor layer in base_model.layers[:fine_tune_at]: layer.trainable = False#fine tune should be done using a lower learning rate to avoid overfittingmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10), metrics=['accuracy'])# note that the trainable parameters changedmodel.summary()base model layers: 175Model: \"model\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 224, 224, 3)] 0 tf.__operators__.getitem (S (None, 224, 224, 3) 0 licingOpLambda) tf.nn.bias_add (TFOpLambda) (None, 224, 224, 3) 0 resnet50 (Functional) (None, 7, 7, 2048) 23587712 global_average_pooling2d (G (None, 2048) 0 lobalAveragePooling2D) dropout (Dropout) (None, 2048) 0 dense (Dense) (None, 1) 2049 =================================================================Total params: 23,589,761Trainable params: 19,454,977Non-trainable params: 4,134,784_________________________________________________________________Continue TrainingHere we fine tune 10 epochs.The accuracy of the validation dataset increased to 1 from the very beginning steps of the fine-tuning process, and the accuracy of the train dataset increased to 1 from the 4th epoch.fine_tune_epochs = 10total_epochs = initial_epochs + fine_tune_epochshistory_fine = model.fit(train_dataset, epochs=total_epochs, initial_epoch=history.epoch[-1], validation_data=validation_dataset)Epoch 30/4010/10 [==============================] - 10s 368ms/step - loss: 0.1937 - accuracy: 0.9426 - val_loss: 0.0682 - val_accuracy: 0.9688Epoch 31/4010/10 [==============================] - 3s 238ms/step - loss: 0.0721 - accuracy: 0.9764 - val_loss: 0.0269 - val_accuracy: 0.9844Epoch 32/4010/10 [==============================] - 3s 237ms/step - loss: 0.0320 - accuracy: 0.9966 - val_loss: 0.0082 - val_accuracy: 1.0000Epoch 33/4010/10 [==============================] - 3s 236ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000Epoch 34/4010/10 [==============================] - 3s 234ms/step - loss: 0.0119 - accuracy: 0.9966 - val_loss: 0.0050 - val_accuracy: 1.0000Epoch 35/4010/10 [==============================] - 3s 236ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000Epoch 36/4010/10 [==============================] - 3s 239ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000Epoch 37/4010/10 [==============================] - 3s 235ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.1148e-04 - val_accuracy: 1.0000Epoch 38/4010/10 [==============================] - 3s 237ms/step - loss: 4.8445e-04 - accuracy: 1.0000 - val_loss: 1.0168e-04 - val_accuracy: 1.0000Epoch 39/4010/10 [==============================] - 3s 238ms/step - loss: 2.3565e-04 - accuracy: 1.0000 - val_loss: 8.9607e-05 - val_accuracy: 1.0000Epoch 40/4010/10 [==============================] - 3s 237ms/step - loss: 1.8288e-04 - accuracy: 1.0000 - val_loss: 1.9273e-05 - val_accuracy: 1.0000Learing CurveAccording to the learing curve, after fine tuning the performance of the model increased significatlly.#visualize the learning curveacc += history_fine.history['accuracy']val_acc += history_fine.history['val_accuracy']loss += history_fine.history['loss']val_loss += history_fine.history['val_loss']plt.figure(figsize=(8, 8))plt.subplot(2, 1, 1)plt.plot(acc, label='Training Accuracy')plt.plot(val_acc, label='Validation Accuracy')plt.ylim([0.5, 1])plt.plot([initial_epochs-1,initial_epochs-1], plt.ylim(), label='Start Fine Tuning')plt.legend(loc='lower right')plt.title('Training and Validation Accuracy')plt.subplot(2, 1, 2)plt.plot(loss, label='Training Loss')plt.plot(val_loss, label='Validation Loss')plt.ylim([0, 1.0])plt.plot([initial_epochs-1,initial_epochs-1], plt.ylim(), label='Start Fine Tuning')plt.legend(loc='upper right')plt.title('Training and Validation Loss')plt.xlabel('epoch')plt.savefig('learning_curve_fine_tuning.png')plt.show()EvaluationAccuracyTest the performance of the model on the test dataset. The model get 0.9250 accuracy on the test dataset.#evaluate on test datasetloss, accuracy = model.evaluate(test_dataset)print('Test accuracy :', accuracy)2/2 [==============================] - 0s 48ms/step - loss: 0.3859 - accuracy: 0.8750Test accuracy : 0.875PredictionPredict on the test datset.test_image = np.empty((0,224,224,3))test_label = []predictions = []for element in test_dataset.as_numpy_iterator(): # Retrieve a batch of images from the test set image_batch, label_batch = element batch_predictions = model.predict_on_batch(image_batch).flatten() # Apply a sigmoid since our model returns logits batch_predictions = tf.nn.sigmoid(batch_predictions) batch_predictions = tf.where(batch_predictions &lt; 0.5, 0, 1) test_image = np.concatenate((test_image, image_batch), axis=0) print(image_batch.shape) #est_image += image_batch test_label += label_batch.tolist() predictions += batch_predictions.numpy().tolist()print(len(predictions))print('Predictions:\\n', predictions)print('Labels:\\n', test_label)(32, 224, 224, 3)(8, 224, 224, 3)40Predictions: [1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]Confusion Matrixcf_matrix = tf.math.confusion_matrix(predictions,test_label)print(cf_matrix)tf.Tensor([[15 0] [ 5 20]], shape=(2, 2), dtype=int32)cf_matrix = tf.math.confusion_matrix(predictions,test_label)group_counts = cf_matrix.numpy().flatten().tolist()print(group_counts)group_names = ['True Neg','False Pos','False Neg','True Pos']print(group_names)group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.numpy().flatten()/np.sum(cf_matrix)]labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]labels = np.asarray(labels).reshape(2,2)sns_heatmap = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')fig = sns_heatmap.get_figure()fig.savefig(\"confusion_matrix.png\") [15, 0, 5, 20]['True Neg', 'False Pos', 'False Neg', 'True Pos']Wrong Pedictionwrong_prediction = [idx for idx, elem in enumerate(predictions) if elem != test_label[idx]]print(\"wrong predict number: \",len(wrong_prediction))print(\"wrong predict index: \", wrong_prediction)wrong predict number: 5wrong predict index: [0, 1, 2, 6, 7]plt.figure(figsize=(10, 10))count = 1for i in wrong_prediction: #print(str(predictions[i]) + \" \" + str(test_label[i])) ax = plt.subplot(len(wrong_prediction)//3+1, 3, count) plt.imshow(test_image[i].astype(\"uint8\")) plt.title(class_names[predictions[i]] + \":\" + class_names[test_label[i]]) plt.axis(\"off\") count+=1#check if test dataset is correct#plt.figure(figsize=(10, 10))#count = 1#for i in range(40):# ax = plt.subplot(4, 10, count)# plt.imshow(test_image[i].astype(\"uint8\"))# plt.title(class_names[test_label[i]])# plt.axis(\"off\")# count+=1More ResultsResults with data augmentationModel ArchitectureThe model with data augmentaion contains a sequential layer.# Model: \"model\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) [(None, 224, 224, 3)] 0 sequential (Sequential) (None, 224, 224, 3) 0 tf.__operators__.getitem (S (None, 224, 224, 3) 0 licingOpLambda) tf.nn.bias_add (TFOpLambda) (None, 224, 224, 3) 0 resnet50 (Functional) (None, 7, 7, 2048) 23587712 global_average_pooling2d (G (None, 2048) 0 lobalAveragePooling2D) dropout (Dropout) (None, 2048) 0 dense (Dense) (None, 1) 2049 =================================================================Total params: 23,589,761Trainable params: 19,454,977Non-trainable params: 4,134,784This is formatted as codeLearning Curve for feature extractionAfter 30 epoches of feature extraction training,loss: 0.4212 - accuracy: 0.8176 - val_loss: 0.3520 - val_accuracy: 0.8906Learning Curve for fine-tuningAfter 10 epoches of fine-tuning, the modelloss: 0.0854 - accuracy: 0.9595 - val_loss: 0.0624 - val_accuracy: 0.9844Wrong predictionConfusion MatrixResults with data augmentaion, without preprocessingModel ArchitectureThe model with data augmentation contains sequential layer, without preprocessing layers (slicing, TF)Model: \"model_1\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_3 (InputLayer) [(None, 224, 224, 3)] 0 sequential (Sequential) (None, 224, 224, 3) 0 resnet50 (Functional) (None, 7, 7, 2048) 23587712 global_average_pooling2d (G (None, 2048) 0 lobalAveragePooling2D) dropout_1 (Dropout) (None, 2048) 0 dense (Dense) (None, 1) 2049 =================================================================Total params: 23,589,761Trainable params: 19,454,977Non-trainable params: 4,134,784# This is formatted as codeLearning Curve for feature extractionThe model after training for feature extraction:loss: 0.4927 - accuracy: 0.7365 - val_loss: 0.4773 - val_accuracy: 0.8125Learning Curve for fine-tuningThe model performance after fine-tuningloss: 0.1380 - accuracy: 0.9426 - val_loss: 0.1083 - val_accuracy: 0.9688 Wrong predictionConfusion Matrix!pip freeze &gt; requirements.txt" }, { "title": "Study Notes of Smplify", "url": "/posts/study-notes-of-smplify/", "categories": "Human_Pose_Estimation", "tags": "", "date": "2023-01-16 14:32:00 -0600", "snippet": "After setting up the environment for Smplify, run the code as followed:conda activate smplpython fit_3d.py ../ --out_dir ../smpl_lsp --vizThis script fits the SMPL model to LSP data, the results were stored in ../smpl_lsp.Let’s go deeper into the source code along with the paper.Data PreparationLoad LSP images and predicted 2D joints by DeepCut CNN stored in “est_joints.npz” including joint 2D position in image coordinate and confidence.Fit SMPL model to a single imagestep 1: Init the modelsAs said in the paper, this project use one of three shape models of SMPL: male, female, and gender-neutral. If gender is known, use the appropriate model.In the script, the gender for each data set was determined by the “lsp_gender.csv” file. Once the gender was determined, the SMPL model and the sph_regs(the regressor model for body shape approximation with capsules) used for interpenetration prevention were determined too. if not use_neutral: gender = 'male' if int(genders[ind]) == 0 else 'female' if gender == 'female': model = model_female if use_interpenetration: sph_regs = sph_regs_female elif gender == 'male model = model_male if use_interpenetration: sph_regs = sph_regs_malestep 2: Create pose prior termThe use of pose prior term is to favor probable poses over improbable ones. By fitting a Mixture Gaussians Model(8 gaussians) to 1 million poses(The GMM over CMU motion capture data), then scoring the new data point(the estimated pose), the GMM can tell the poses that are significantly different from the rest of the data. In English, the more common pose will have a higher score(the probability distribution function value). In the equation, its negative logarithm will be closer to 0. The GMM model was stored in smpl_public/code/models/gmm_08.pklThe mean pose was used to be the initial pose for the optimization process.step 3: Estimate the camera translation and body orientationThe camera translation and body orientation were optimized based on torso joints including hips and shoulders.The camera rotation was set as an identity, in my opinion, this is because the rotation of the camera and the body orientation might be biased.The focal length of the camera was fixed to 5000, and not optimized because the problem will be too unconstrained to optimize it together with camera translation.Specifically, the depth of camera translation was first initialized via a similar triangle of torso joints.Then project the SMPL joints to the image using the camera with initialized depth, rotation and given focal length.An optimization function is here to optimize camera translation and body orientation which minimizes the distance between predicted 2D joints and estimated 2D joints on torso joints.A regularizer for the camera translation is to constrain the change of the camera translation.# optimize for camera translation and body orientation free_variables = [cam.t, opt_pose[:3]] ch.minimize( # data term defined over torso joints... {'cam': j2d[torso_cids] - cam[torso_smpl_ids], # ...plus a regularizer for the camera translation 'cam_t': 1e2 * (cam.t[2] - init_t[2])}, x0=free_variables, method='dogleg', callback=on_step, options={'maxiter': 100, 'e_3': .0001, # disp set to 1 enables verbose output from the optimizer 'disp': 0})In the beginning, the camera’s translation was initialized by similar triangle, the body orientation was initialized by mean pose.After 11 iterations for this image, the optimization function converged.Another return of this initialize_camera() function was the try_both_orient boolean variable. This was determined by the pixel distance between the predicted shoulder 2D joints.# check how close the shoulder joints are try_both_orient = np.linalg.norm(j2d[8] - j2d[9]) &lt; pix_thshStep 4: Optimize body pose and shapeThis is the core optimization of the pose estimate process, here will fit the model to the given set of 2D joints with the given estimated camera and body orientation. If the flag try_both_orient is true, the model will be fit twice for the body orientation and flipped body orientation, in the end, choose the one with the lower error as the final estimated pose.The code for optimization is listed below:# run the optimization in 4 stages, progressively decreasing the # weights for the priors for stage, (w, wbetas) in enumerate(opt_weights): _LOGGER.info('stage %01d', stage) objs = {} objs['j2d'] = obj_j2d(1., 100) objs['pose'] = pprior(w) objs['pose_exp'] = obj_angle(0.317 * w) objs['betas'] = wbetas * betas if regs is not None: objs['sph_coll'] = 1e3 * sp ch.minimize( objs, x0=[sv.betas, sv.pose], method='dogleg', callback=on_step, options={'maxiter': 100, 'e_3': .0001, 'disp': 0}) Data Term - objs[‘j2d’]: distance between observed and estimated joints in 2D, like the previous step, the goal is to minimize the distance between predicted 2D joints and the projected SMPL joints. # data term: distance between observed and estimated joints in 2D obj_j2d = lambda w, sigma: ( w * weights.reshape((-1, 1)) * GMOf((j2d[cids] - cam[smpl_ids]), sigma)) The equation is shown in the image below,\\ Prior Pose Term - objs[‘pose’]: Here the GMM created in step 2 was used\\ Joint angles pose prior term for elbow and knee - objs[‘pose_exp’]: to penalty unnatural pose of elbow and knee # joint angles pose prior, defined over a subset of pose parameters: # 55: left elbow, 90deg bend at -np.pi/2 # 58: right elbow, 90deg bend at np.pi/2 # 12: left knee, 90deg bend at np.pi/2 # 15: right knee, 90deg bend at np.pi/2 alpha = 10 my_exp = lambda x: alpha * ch.exp(x) obj_angle = lambda w: w * ch.concatenate([my_exp(sv.pose[55]), my_exp(-sv.pose[ 58]), my_exp(-sv.pose[12]), my_exp(-sv.pose[15])]) Interpenetration error term - objs[‘sph_coll’]: to penalty interpenetration if regs is not None: # interpenetration term sp = SphereCollisions( pose=sv.pose, betas=sv.betas, model=model, regs=regs) sp.no_hands = True Shape prior term - objs[‘betas’]: to penalty shape parameters derivated from the mean shape." }, { "title": "Smplify python2.7 with Ubuntu22.04.1 and Anaconda", "url": "/posts/smplify-python2-7-with-ubuntu22-04-1-and-anaconda/", "categories": "Technical-Notes", "tags": "", "date": "2023-01-06 13:56:00 -0600", "snippet": "According to the README Getting Started, after extracting the code, we should get LSP data, but the LSP dataset cannot be downloaded from the given path, I found a source from the other website. Get if from here LSPThe other dependencies are the same as the SMPL.Follow the README, created a symbolic link to LSP images, and a symlink to the SMPL model from the SMPL package.Before running the fit_3d.py, the name of the model should be changed.line 665-669, the model path should be changed to the name of SMPL models.Finally, smplify is good to go." }, { "title": "SMPL python2.7 with Ubuntu22.04.1 and Anaconda", "url": "/posts/smpl-python2-7-with-ubuntu22-04-1-and-anaconda/", "categories": "Technical-Notes", "tags": "", "date": "2023-01-04 13:43:00 -0600", "snippet": "After some effort in trying to run SMPL python on macOS and failing, I decided to use Ubuntu.Install Ubuntu dual system with Windows on my PC in my lab is another story.Here I wrote down the process for future usage. I referenced a lot from this post in Chinese is very detailed, however, need some updates on some detail. ReferenceEnvironment:Ubuntu22.04.1Anaconda python2.7Step1Create a new environment through anaconda with python 2 and activateconda create -n smpl python=2conda activate smplStep2Install required dependenciespip install numpypip install scipypip install chumpyThe trick for installing the correct version of opencv is using the following command. After running render_smpl.py found out the opencv I installed is not correct. It has to be 4.2.0.32, the last version supporting python2.7. I also tried pip, but not correct.pip2 install opencv-python==4.2.0.32For the installation of opendr, is much easier than the thing on macOS.sudo apt install libosmesa6-devsudo apt-get install build-essentialsudo apt-get install libgl1-meas-devsudo apt-get install libglu1-meas-devsudo apt-get install freeglut3-devpip install opendrStep3Set the python package path for smpl. Go to the python installed path, mine is anaconda3/envs/smpl/lib/python2.7/site-packages Create .pth file touch xxx.pth Add the absolute path of smpl folder and smpl_websuer folder which contain init.py file. /home/jiechang/Documents/SMPL/SMPL/smpl/home/jiechang/Documents/SMPL/SMPL/smpl/smpl_webuser Step4Test hello_smpl.pyneed to change the model’s name in the script If successful there will be a hello_smpl.obj model in the path.line 48m = load_model( '../../models/basicmodel_f_lbs_10_207_0_v1.1.0.pkl' )Step5Test render_smpl.py.Still need to change the name of the model accordingly.Here, another things need to fix is,if encounter ImportError: /home/jiechang/anaconda3/envs/smpl/lib/python2.7/site-packages/../../libstdc++.so.6: version ‘GLIBCXX_3.4.30’ not found (required by /lib/x84_64-linux-gnu/libLLVM-13.so.1)Solution:strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXXThe version of 3.4.30 should be shown, the lib is exist in our system, but the anaconda don’t know, we need to create a shortcut to let anaconda find the lib.ln -sf /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /home/jiechang/anaconda3/envs/smpl/bin/../lib/libstdc++.so.6Finally get hello smpl to work. So I can go and learn Smplify." }, { "title": "Leetcode 141 Linked List Cycle", "url": "/posts/leetcode-141-linked-list-cycle/", "categories": "Leetcode-Diary", "tags": "", "date": "2023-01-03 15:42:00 -0600", "snippet": "141-Linked List CycleGiven head, the head of a linked list, determine if the linked list has a cycle in it.There is a cycle in a linked list if there is some node in the list that can be reached again by continuously following the next pointer. Internally, pos is used to denote the index of the node that tail’s next pointer is connected to. Note that pos is not passed as a parameter.Return true if there is a cycle in the linked list. Otherwise, return false.method 1Store visited node in hash-table{ class Solution {public: bool hasCycle(ListNode *head) { if(!head) return false; unordered_set&lt;ListNode*&gt; visited; ListNode* node = head; visited.insert(node); while(node-&gt;next) { if(visited.find(node-&gt;next)!=visited.end()) { return true; } node = node-&gt;next; visited.insert(node); } return false; }};}method 2This method is genius. Called “Floyd’s Tortoise and Hare”, an algorithm using two pointers with different speeds to check a cycle. The idea is simple, image Hare and Tortoise are racing, they must meet each other at some point if they are running in a circle.And this video is so funny. {class Solution {public: bool hasCycle(ListNode *head) { if(!head) return false; ListNode* slow = head; ListNode* fast = head; while(slow &amp;&amp; fast) { //slow go one step slow = slow-&gt;next; //fast go two step fast = fast-&gt;next; if(fast) { fast = fast-&gt;next; } if(!slow || !fast) { return false; } if(slow == fast) { return true; } } return false; }};}" }, { "title": "Leetcode 110 Balanced Binary Tree", "url": "/posts/leetcode-110-balanced-binary-tree/", "categories": "Leetcode-Diary", "tags": "", "date": "2023-01-03 15:18:00 -0600", "snippet": "110-Balanced Binary TreeGiven a binary tree, determine if it is height-balanced.Height-BalancedA height-balanced binary tree is a binary tree in which the depth of the two subtrees of every node never differs by more than one.Method 1Intuitively check each node and calculate the height of the left and the right subtree. This method is a Top-down method, containing duplicate calculations for the height of subtrees.I was stuck at the calculation of the height of the tree. For calculating the height, the height of an empty tree is -1. For each node, we check its left and right to get the largest height from the subtree, and take the node itself into height.{class Solution {public: int depth(TreeNode* root) { if(root == nullptr) { return -1; } return 1 + max(depth(root-&gt;left), depth(root-&gt;right)); } bool isBalanced(TreeNode* root) { if(!root) return true; if(abs(depth(root-&gt;left) - depth(root-&gt;right)) &gt; 1) return false; return isBalanced(root-&gt;left) &amp;&amp; isBalanced(root-&gt;right); }};}Method 2To avoid extra computation on the height of the subtrees, we can remember the height of the child and check if a child’s subtree is balanced.{class Solution {public: bool isBalancedHelper(TreeNode* root, int &amp;height) { //empty tree is balanced if(root == nullptr) { height = -1; return true; } //check children int left, right = -1; //left subtree and right subtree should be balanced if(isBalancedHelper(root-&gt;left, left) &amp;&amp; isBalancedHelper(root-&gt;right, right) &amp;&amp; abs(left-right)&lt;=1) { //update current node height height = max(left,right) + 1; return true; } return false; } bool isBalanced(TreeNode* root) { int height = -1; return isBalancedHelper(root,height); }};}" }, { "title": "Leetcode 235 Lowest Common Ancestor of a Binary Search Tree", "url": "/posts/leetcode-235-lowest-common-ancestor-of-a-binary-search-tree/", "categories": "Leetcode-Diary", "tags": "", "date": "2023-01-03 14:51:00 -0600", "snippet": "235-Lowest Common Ancestor of a Binary Search TreeGiven a binary search tree (BST), find the lowest common ancestor (LCA) node of two given nodes in the BST.According to the definition of LCA on Wikipedia: “The lowest common ancestor is defined between two nodes p and q as the lowest node in T that has both p and q as descendants (where we allow a node to be a descendant of itself).”NotesBinary Search Tree(BST)Binary Search Tree is a node-based binary tree data structure which has the following properties:The left subtree of a node contains only nodes with keys lesser than the node’s key.The right subtree of a node contains only nodes with keys greater than the node’s key.The left and right subtree each must also be a binary search tree.For a problem related to a tree, it is intuitive to think about divide and conquer and solve the problem recursively.The lowest common ancestor should have the property that p and q are on different sides of the node. for simplicity, let p be smaller than q, if the node’s value satisfies p &lt;= node &lt;= q, then the node is the lowest common ancestor.Three conditions: The value of root is smaller than p and q, check the left subtree The value of root is bigger than p and q, check the right subtree The value of root satisfies p &lt;= node &lt;= q{ class Solution {public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { if((p-&gt;val &lt;= root-&gt;val &amp;&amp; q-&gt;val &gt;= root-&gt;val) || (q-&gt;val &lt;= root-&gt;val &amp;&amp; p-&gt;val &gt;= root-&gt;val)) { return root; } else if(p-&gt;val &gt; root-&gt;val &amp;&amp; q-&gt;val &gt; root-&gt;val) { return lowestCommonAncestor(root-&gt;right,p,q); } else if(p-&gt;val &lt; root-&gt;val &amp;&amp; q-&gt;val &lt; root-&gt;val) { return lowestCommonAncestor(root-&gt;left,p,q); } return nullptr; }};}Actually, as the description, p and q must exist in the tree, so the LCA must exist too, the three conditions can change to The value of root is smaller than p and q, check the left subtree The value of root is bigger than p and q, check the right subtree The root must be LCAThe code can be cleaner.{ class Solution {public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { if(p-&gt;val &gt; root-&gt;val &amp;&amp; q-&gt;val &gt; root-&gt;val) { return lowestCommonAncestor(root-&gt;right,p,q); } else if(p-&gt;val &lt; root-&gt;val &amp;&amp; q-&gt;val &lt; root-&gt;val) { return lowestCommonAncestor(root-&gt;left,p,q); } else { return root; } }};}" }, { "title": "Leetcode 121 Best Time to Buy and Sell Stock", "url": "/posts/leetcode-121-best-time-to-buy-and-sell-stock/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-06 12:29:00 -0500", "snippet": "Best Time to Buy and Sell Stock problem resultsBest Time to Buy and Sell StockFirst Blood{ class Solution {public: int maxProfit(vector&lt;int&gt;&amp; prices) { int lessest = INT_MAX; int max = 0; int curr = 0; for(int i = 0; i &lt; prices.size(); i++) { if(prices[i] &lt; lessest) { lessest = prices[i]; } curr = prices[i] - lessest; if(curr &gt; max) { max = curr; } } return max; }};}Double Kill{class Solution {public: int maxProfit(vector&lt;int&gt;&amp; prices) { int lessest = INT_MAX; int max_profit = 0; for(int i = 0; i &lt; prices.size(); i++) { lessest = min(lessest,prices[i]); max_profit = max(max_profit,prices[i] - lessest); } return max_profit; }};}" }, { "title": "Leetcode 409 Longest Palindrome", "url": "/posts/leetcode-409-longest-palindrome/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-06 12:28:00 -0500", "snippet": "Longest Palindrome problem resultsLongest PalindromeGiven a string s which consists of lowercase or uppercase letters, return the length of the longest palindrome that can be built with those letters.Letters are case sensitive, for example, “Aa” is not considered a palindrome here.Example 1:Input: s = “abccccdd”Output: 7Explanation: One longest palindrome that can be built is “dccaccd”, whose length is 7.Example 2:Input: s = “a”Output: 1Explanation: The longest palindrome that can be built is “a”, whose length is 1.First BloodA palindrome string can be formed by several pairs of a character and one single character which is optional.So we can simply calculate the number of each character to check how many pairs and if there is a single character that can be served as the palindrome top.{ class Solution {public: int longestPalindrome(string s) { unordered_map&lt;char,int&gt; counter; //count the number of each character occured in the given string for(int i =0; i&lt;s.size(); i++) { if(counter.find(s[i]) != counter.end()) { counter[s[i]]++; } else { counter[s[i]] = 1; } } int paired_count = 0; int single_count = 0; //traverse each pair in the map, to check each character's number is odd or even. for(unordered_map&lt;char,int&gt;::iterator iter = counter.begin(); iter != counter.end(); iter++) { int val = iter-&gt;second; if(val%2 == 0) { paired_count += val; } else { paired_count += val-1; single_count++; } } if(single_count != 0) return paired_count+1; else return paired_count; }};}Double Kill{class Solution {public: int longestPalindrome(string s) { int counter[128] = {}; for(int i =0; i&lt;s.size(); i++) { counter[s[i]]++; } int paired_count = 0; int single_count = 0; for(int i = 0; i &lt; 128; i++) { int val = counter[i]; if(val%2 == 0) { paired_count += val; } else { paired_count += val-1; single_count++; } } if(single_count != 0) return paired_count+1; else return paired_count; }};}Triple Kill{class Solution {public: int longestPalindrome(string s) { int counter[58] = {0}; for(char c : s) { counter[c -'A']++; } int paired_count = 0; bool is_odd = false; for(int val : counter) { if(val%2 == 0) { paired_count += val; } else { paired_count += val-1; is_odd = true; } } return is_odd ? paired_count+1 : paired_count; }};}" }, { "title": "Leetcode 589 N-ary Tree Preorder Traversal", "url": "/posts/leetcode-589-n-ary-tree-preorder-traversal/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-06 12:25:00 -0500", "snippet": "N-ary Tree Preorder Traversal problem resultsN-ary Tree Preorder TraversalGiven the root of an n-ary tree, return the preorder traversal of its nodes’ values.Nary-Tree input serialization is represented in their level order traversal. Each group of children is separated by the null value (See examples)First BloodVisit each node’s children recursively.{ /*// Definition for a Node.class Node {public: int val; vector&lt;Node*&gt; children; Node() {} Node(int _val) { val = _val; } Node(int _val, vector&lt;Node*&gt; _children) { val = _val; children = _children; }};*/class Solution {public: void preorder(Node* head, vector&lt;int&gt;&amp; op) { op.push_back(head-&gt;val); for(int i = 0; i &lt; head-&gt;children.size(); i++) { preorder(head-&gt;children[i],op); } } vector&lt;int&gt; preorder(Node* root) { vector&lt;int&gt; op; if(root == nullptr) return op; op.push_back(root-&gt;val); for(int i = 0; i &lt; root-&gt;children.size(); i++) { preorder(root-&gt;children[i],op); } return op; }};}Double KillThe code could be simpler.{class Solution {public: void pre(Node* head, vector&lt;int&gt;* op) { if(head) { op-&gt;push_back(head-&gt;val); for(int i = 0; i &lt; head-&gt;children.size(); i++) { pre(head-&gt;children[i],op); } } } vector&lt;int&gt; preorder(Node* root) { vector&lt;int&gt; op; if(root) { pre(root,&amp;op); } return op; }};}" }, { "title": "Leetcode 704 Binary Search", "url": "/posts/leetcode-704-binary-search/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-06 12:24:00 -0500", "snippet": "Binary Search problem resultsBinary SearchGiven an array of integer nums which is sorted in ascending order, and an integer target, write a function to search target in nums. If the target exists, then return its index. Otherwise, return -1.You must write an algorithm with O(log n) runtime complexity.Keywords for binary search: ordered array, O(log n) runtime complexity.Algorithm:left pointer points to the first element in the array, and the right pointer points to the last element in the array. Check if the target matches the middle element of left and right (temp = left + (right-left)/2), if the target is less than the temp element, the target must be on the right side of the temp element, so move the left pointer to the next element of temp. else if the target is bigger than the temp element, the target must be on the left side of the temp element, so move the right pointer to the prior element of temp, until we find target == temp element or the left pointer meet the right pointer.First Blood{ class Solution {public: int search(vector&lt;int&gt;&amp; nums, int target) { int left = 0; int right = nums.size()-1; while(left&lt;=right) { int temp = (left+right)/2; if(nums[temp] == target) { return temp; } else if(nums[temp] &lt; target) { left = temp + 1; } else if(nums[temp] &gt; target) { right = temp - 1; } } return -1; }};}Double Kill{ class Solution {public: int search(vector&lt;int&gt;&amp; nums, int target) { int left = 0; int right = nums.size()-1; while(left&lt;=right) { //(left+right)/2 should be care, if right and left is too big int temp = left + (right - left)/2; if(nums[temp] == target) { return temp; } else if(nums[temp] &lt; target) { left = temp + 1; } else if(nums[temp] &gt; target) { right = temp - 1; } } return -1; }};}" }, { "title": "Leetcode 102 Binary Tree Level Order Traversal", "url": "/posts/leetcode-102-binary-tree-level-order-traversal/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-06 12:24:00 -0500", "snippet": "Binary Tree Level Order Traversal problem resultsBinary Tree Level Order TraversalFirst Blood{ /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */class Solution {public: vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) { queue&lt;TreeNode*&gt; node_queue; vector&lt;vector&lt;int&gt;&gt; result; if(root) { node_queue.push(root); } while(node_queue.size()) { vector&lt;int&gt; cur; int level = node_queue.size(); for(int i = 0; i &lt; level; i++) { TreeNode* node = node_queue.front(); cur.push_back(node-&gt;val); node_queue.pop(); if(node-&gt;left) { node_queue.push(node-&gt;left); } if(node-&gt;right) { node_queue.push(node-&gt;right); } } result.push_back(cur); } return result; }};}" }, { "title": "Leetcode 98 Validate Binary Search Tree", "url": "/posts/leetcode-98-validate-binary-search-tree/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-06 12:22:00 -0500", "snippet": "Validate Binary Search Tree problem resultsValidate Binary Search TreeFirst Blood{ /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */class Solution {public: bool isBST(TreeNode* node,TreeNode* min_node,TreeNode* max_node) { if(!node) return true; if(min_node) { if(node-&gt;val &lt;= min_node-&gt;val) { return false; } } if(max_node) { if(node-&gt;val &gt;= max_node-&gt;val) { return false; } } return isBST(node-&gt;left,min_node,node) &amp;&amp; isBST(node-&gt;right,node,max_node); } bool isValidBST(TreeNode* root) { return isBST(root,nullptr,nullptr); }};}" }, { "title": "Leetcode 278 First Bad Version", "url": "/posts/leetcode-278-first-bad-version/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-06 12:22:00 -0500", "snippet": "First Bad Version problem resultsFirst Bad VersionFirst Blood{ // The API isBadVersion is defined for you.// bool isBadVersion(int version);class Solution {public: int firstBadVersion(int n) { int left = 0; int right = n-1; while(left &lt;= right) { int pivot = left + (right - left)/2; if(isBadVersion(pivot + 1)) { //check if is first bad version if(isBadVersion(pivot)) { right = pivot - 1; } else { return pivot + 1; } } else { left = pivot + 1; } } return -1; }};}" }, { "title": "Leetcode 876 Middle of the Linked List", "url": "/posts/leetcode-876-middle-of-the-linked-list/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-01 15:12:00 -0500", "snippet": "Middle of the Linked List problem resultsMiddle of the Linked ListFirst BloodSaving each node in a vector, calculate the middle index, and get the node in the vector.{ ListNode* middleNode(ListNode* head) { vector&lt;ListNode*&gt; node_list; node_list.push_back(head); while(head-&gt;next != nullptr) { head = head-&gt;next; node_list.push_back(head); } int index = node_list.size()/2; return node_list[index]; }}" }, { "title": "Leetcode 142 Linked List Cycle II", "url": "/posts/leetcode-142-linked-list-cycle-ii/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-01 15:12:00 -0500", "snippet": "Linked List Cycle II problem resultsLinked List Cycle IIFirst BloodSaving the visited node in an unordered_set, stop when meeting the same node.{ ListNode *detectCycle(ListNode *head) { if(head == nullptr) return head; unordered_set&lt;ListNode*&gt; visited_node; while(head-&gt;next != nullptr &amp;&amp; visited_node.find(head-&gt;next) == visited_node.end()) { visited_node.insert(head); head = head-&gt;next; } return head-&gt;next; }}" }, { "title": "Leetcode 0206 Reverse Linked List", "url": "/posts/leetcode-0206-reverse-linked-list/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-01 15:11:00 -0500", "snippet": "Reverse Linked List problem resultsReverse Linked ListFirst BloodIterative Method.{ ListNode* reverseList(ListNode* head) { if(head == nullptr || head-&gt;next == nullptr) return head; ListNode* curr = head-&gt;next; ListNode* temp = curr-&gt;next; curr-&gt;next = head; head-&gt;next = nullptr; head = curr; curr = temp; while(curr != nullptr) { temp = curr-&gt;next; curr-&gt;next = head; head = curr; curr = temp; } return head; }}Double KillIterative Method with simpler code.{ ListNode* reverseList(ListNode* head) { ListNode* temp = nullptr; ListNode* prev = nullptr; while(head != nullptr) { temp = head-&gt;next; head-&gt;next = prev; prev = head; head = temp; } return prev; }}Triple KillRecursive Method.{ ListNode* reverseList(ListNode* head) { if(head == NULL || head -&gt; next == NULL){ return head; } ListNode* reversedListHead = reverseList(head -&gt; next); head -&gt; next -&gt; next = head; head -&gt; next = NULL; return reversedListHead; }}" }, { "title": "Leetcode 0021 Merge Two Sorted Lists", "url": "/posts/leetcode-0021-merge-two-sorted-lists/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-08-01 15:11:00 -0500", "snippet": "Merge Two Sorted Lists problem resultsMerge Two Sorted ListsI should try a recursive method later.First BloodIterative method.{ ListNode* mergeTwoLists(ListNode* list1, ListNode* list2) { if(list1 == nullptr) { return list2; } if(list2 == nullptr) { return list1; } ListNode* result = nullptr; if(list1-&gt;val &lt;= list2-&gt;val) { result = list1; list1 = list1-&gt;next; } else { result = list2; list2 = list2-&gt;next; } ListNode* temp = result; while(list1 != nullptr &amp;&amp; list2 != nullptr) { if(list1-&gt;val &lt;= list2-&gt;val) { temp-&gt;next = list1; list1 = list1-&gt;next; } else { temp-&gt;next = list2; list2 = list2-&gt;next; } temp = temp-&gt;next; } if(list1 != nullptr) { temp-&gt;next = list1; } if(list2 != nullptr) { temp-&gt;next = list2; } return result; }}" }, { "title": "Leetcode 0392 Is Subsequence", "url": "/posts/leetcode-0392-is-subsequence/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-07-28 11:53:00 -0500", "snippet": "Is Subsequence problem resultsIs SubsequenceFirst BloodEasiest method. Two-pointer.Traversing each string, update the index of “s” when finding the same character in t, else only update the index of “t”.{ bool isSubsequence(string s, string t) { int j = 0; for(int i = 0; i &lt; t.length() &amp;&amp; j &lt; s.length(); i++) { if(t[i] == s[j]) j++; } if(j == s.length()) { return true; } else { return false; } }}Double KillUsing a recursive method. The idea behind this method is similar to the first method. By recursively checking if the last characters of the strings are matching.{ bool isSubs(string&amp; s, string&amp; t, int m, int n) { if(m == 0) return true; if(n == 0) return false; // If last characters of two // strings are matching if (s[m - 1] == t[n - 1]) return isSubs(s, t, m - 1, n - 1); // If last characters are // not matching return isSubs(s, t, m, n - 1); } bool isSubsequence(string s, string t) { return isSubs(s,t,s.length(),t.length()); }}Triple KillUsing the longest common subsequence method and dynamic programming.This method is not straightforward as the first two methods, but it offers another way to solve this problem, and also learn the knowledge of the lcs and the dp.{ int longestCommonSubsequence(string&amp; s, string&amp; t) { int m = s.length(), n = t.length(); vector&lt;vector&lt;int&gt;&gt; dp(m + 1, vector&lt;int&gt;(n + 1, 0)); for (int i = 1; i &lt;= m; ++i) { for (int j = 1; j &lt;= n; ++j) { dp[i][j] = (s[i-1] == t[j-1]) ? dp[i-1][j-1] + 1 : max(dp[i][j-1], dp[i-1][j]); } } return dp[m][n]; } bool isSubsequence(string s, string t) { return s.length() == longestCommonSubsequence(s,t); }}" }, { "title": "Leetcode 0205 Isomorphic Strings", "url": "/posts/leetcode-0205-isomorphic-strings/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-07-28 11:52:00 -0500", "snippet": "Isomorphic String problem resultsIsomorphic StringI can find an excuse this time for my multiple failures. I had a fever that time after getting the booster of Covid19. My mind is not clear.First BloodMy first accepted solution is straightforward.s1=s,t1=tMapping each character in “s” to “t” and converting s1 with the map.Mapping each character in “t” to “s” and converting t1 with the map.compare s1 with t, t1 with s.Simply test two ways.{ bool isIsomorphic(string s, string t) { int size = s.length(); map&lt;char,char&gt; replaced_char_s; map&lt;char,char&gt; replaced_char_t; string s1 = s; string t1 = t; for(int i = 0; i &lt; size; i++) { if(replaced_char_s.find(s[i]) == replaced_char_s.end()) { replaced_char_s[s[i]] = t[i]; } if(replaced_char_t.find(t[i]) == replaced_char_t.end()) { replaced_char_t[t[i]] = s[i]; } if(replaced_char_s.find(s[i]) != replaced_char_s.end()) { s1[i] = replaced_char_s[s[i]]; } if(replaced_char_t.find(t[i]) != replaced_char_t.end()) { t1[i] = replaced_char_t[t[i]]; } } if(s1 == t &amp;&amp; t1 == s) { return true; } else { return false; } }}Double KillMy second try was smarter.There’s no need for me to convert the string to check the equality, what I can do is simply map and check if the two-way mapping is correct.If no mapping exists in either of the map, map it.If a mapping exists in the s_to_t map, check if the existing mapping is equal to the current t[i], if not return false and vice versa.{ bool isIsomorphic(string s, string t) { int size = s.length(); map&lt;char,char&gt; s_2_t; map&lt;char,char&gt; t_2_s; for(int i = 0; i &lt; size; i++) { if(s_2_t.find(s[i]) == s_2_t.end() &amp;&amp; t_2_s.find(t[i]) == t_2_s.end()) { s_2_t[s[i]] = t[i]; t_2_s[t[i]] = s[i]; } else if(s_2_t.find(s[i]) != s_2_t.end() &amp;&amp; t_2_s.find(t[i]) == t_2_s.end()) { if(s_2_t[s[i]] != t[i]) return false; } else if(s_2_t.find(s[i]) == s_2_t.end() &amp;&amp; t_2_s.find(t[i]) != t_2_s.end()) { if(t_2_s[t[i]] != s[i]) return false; } else { if(s_2_t[s[i]] != t[i] || t_2_s[t[i]] != s[i]) return false; } } return true; }}Triple KillThe last solution I tried is to transform both string into a template string, and check if the template string is the same.For each character in the given string, we replace it with the index of that character’s first occurrence in the string.{ bool isIsomorphic(string s, string t) { unordered_map&lt;char,int&gt; s_2_format; unordered_map&lt;char,int&gt; t_2_format; for(int i = 0; i &lt; s.length(); i++) { if(s_2_format.find(s[i]) == s_2_format.end()) { s_2_format[s[i]] = s_2_format.size(); } s[i] = s_2_format[s[i]]; if(t_2_format.find(t[i]) == t_2_format.end()) { t_2_format[t[i]] = t_2_format.size(); } t[i] = t_2_format[t[i]]; if(s[i] != t[i]) { return false; } } return true; }}" }, { "title": "Leetcode 1480 Running Sum of 1d Array", "url": "/posts/leetcode-1480-running-sum-of-1d-array/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-07-25 14:54:00 -0500", "snippet": "Running Sum of 1d Array problem resultsRunning Sum of 1d ArrayFirst BloodMy first solution is so straightforward, that I created a new vector to save the running sum{ vector&lt;int&gt; runningSum(vector&lt;int&gt;&amp; nums) { vector&lt;int&gt; runningSum(nums.size()); runningSum[0] = nums[0]; for(int i = 1; i &lt; nums.size(); i++) { runningSum[i] = runningSum[i-1] + nums[i]; } return runningSum; }}Double KillIt could be better, I can return the result with the input array.{ vector&lt;int&gt; runningSum(vector&lt;int&gt;&amp; nums) { for(int i = 1; i &lt; nums.size(); i++) { nums[i] += nums[i-1]; } return nums; }}" }, { "title": "Leetcode 0724 Find Pivot Index", "url": "/posts/leetcode-0724-find-pivot-index/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-07-25 14:52:00 -0500", "snippet": "Find Pivot Index problem resultsFind Pivot IndexThe pivot index is the index where the sum of all the numbers strictly to the left of the index is equal to the sum of all the numbers strictly to the index’s right.This problem is related to the running sum of 1d array.First BloodI used a silly brute force method but failed to be accepted. For each element in the vector, I calculated the left sum and right sum to check the equality.{ int pivotIndex(vector&lt;int&gt;&amp; nums) { int left_sum = 0; int right_sum = 0; for(int i = 0; i &lt; nums.size(); i++) { //calculate left sum for(int j = 0; j &lt; i; j++) { left_sum += nums[j]; } //calculate right sum for(int k = i+1; k &lt; nums.size(); k++) { right_sum += nums[k]; } if(left_sum == right_sum) { return i; } left_sum = 0; right_sum = 0; } return -1; }}Double KillMy second try was calculating the sum of the given array, for each element in the array, calculate the left sum, then right sum = sum - left_sum - nums[i].This passed the test but still with a bad performance.{int pivotIndex(vector&lt;int&gt;&amp; nums) { int left_sum = 0; int right_sum = 0; int sum = 0; //calculate sum for(int i = 0; i &lt; nums.size(); i++) { sum += nums[i]; } for(int i = 0; i &lt; nums.size(); i++) { //calculate left sum for(int j = 0; j &lt; i; j++) { left_sum += nums[j]; } right_sum = sum - left_sum - nums[i]; if(left_sum == right_sum) { return i; } left_sum = 0; right_sum = 0; } return -1; }}Triple KillAfter checking the Discussion, I got an acceptable submission.By calculating the left_sum as the running sum of the given array.{ int pivotIndex(vector&lt;int&gt;&amp; nums) { int left_sum = 0; int right_sum = 0; int sum = 0; //calculate sum for(int i = 0; i &lt; nums.size(); i++) { sum += nums[i]; } for(int i = 0; i &lt; nums.size(); i++) { right_sum = sum - left_sum - nums[i]; if(left_sum == right_sum) { return i; } //calculate running sum left_sum += nums[i]; } return -1; }}" }, { "title": "Leetcode List", "url": "/posts/leetcode-list/", "categories": "Leetcode-Summary", "tags": "", "date": "2022-07-01 11:05:00 -0500", "snippet": " No Link 0001 Two Sum 0002 Add Two Numbers 0003 Longest Substring without Repeating Characters 0004 Median of Two Sorted Arrays 0033 Search in Rotated Sorted Array " }, { "title": "Leetcode 0033 Search in Rotated Sorted Array", "url": "/posts/leetcode-0033-search-in-rotated-sorted-array/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-07-01 10:40:00 -0500", "snippet": "Search in Rotated Sorted Array problem resultsSearch in Rotated Sorted ArrayFirst BloodI got a wrong answer because I forget to check if the pivot number is equal to the target.Double KillFirst, find the pivot index, if no pivot index was found, the array is not rotated, do a binary search on the whole array.Then check the target with the pivot number, to decide to do a binary search on which part of the array.{ class Solution {public: int search(vector&lt;int&gt;&amp; nums, int target) { int pivot = nums[0]; int size = nums.size(); if(size == 1) { return target == pivot ? 0 : -1; } int i = 0; //find pivot index for(i = 0; i &lt; size; i++) { if(nums[i] &lt; pivot) { break; } } int low = 0; int high = 0; //the array is not rotated if(i == size) { //Do binary search on whole array low = 0; high = size -1; } else { //pivot index is i-1; //check target on which side of pivot index if(target &gt;= pivot) { low = 0; high = i-1; } else { low = i; high = size-1; } } int j = 0; while(low&lt;=high) { j = (low + high)/2; if(nums[j] == target) { return j; } else if(nums[j] &lt; target) { low = j + 1; } else { high = j - 1; } } return -1; }};}" }, { "title": "String", "url": "/posts/string/", "categories": "Leetcode-Summary", "tags": "", "date": "2022-06-30 14:31:00 -0500", "snippet": "" }, { "title": "Recursion", "url": "/posts/recursion/", "categories": "Leetcode-Summary", "tags": "", "date": "2022-06-30 14:30:00 -0500", "snippet": " No Link 0002 Add Two Numbers " }, { "title": "Binary Search", "url": "/posts/binary-search/", "categories": "Leetcode-Summary", "tags": "", "date": "2022-06-30 14:30:00 -0500", "snippet": " No Link 0004 Median of Two Sorted Arrays 0033 Search in Rotated Sorted Array " }, { "title": "Hash Table", "url": "/posts/hash-table/", "categories": "Leetcode-Summary", "tags": "", "date": "2022-06-30 14:29:00 -0500", "snippet": " No Link 0001 Two Sum 0003 Longest Substring without Repeating Characters " }, { "title": "Linked List", "url": "/posts/linked-list/", "categories": "Leetcode-Summary", "tags": "", "date": "2022-06-30 14:28:00 -0500", "snippet": " No Link 0002 Add Two Numbers " }, { "title": "Array", "url": "/posts/array/", "categories": "Leetcode-Summary", "tags": "", "date": "2022-06-30 14:24:00 -0500", "snippet": " No Link 0001 Two Sum 0004 Median of Two Sorted Arrays " }, { "title": "Leetcode 0004 Median of Two Sorted Arrays", "url": "/posts/leetcode-0004-median-of-two-sorted-arrays/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-06-24 11:19:00 -0500", "snippet": "Median of Two Sorted Arrays problem resultsMedian of Two Sorted ArraysFirst BloodMy first solution is so brute force and the run time complexity is not O(log(m+n)). But I think this solution is more straightforward to come up with. It could be better because there’s no need to sort the whole array, it can be stopped when half of the elements are sorted.In other words, stop looping when the median is found.{double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) { int num1_size = nums1.size(); int num2_size = nums2.size(); int size = num1_size + num2_size; int vec[size]; int i = size-1; int m = num1_size-1; int n = num2_size-1; while(i &gt;= 0) { if(m&lt;0) { vec[i] = nums2[n]; n--; } else if(n&lt;0) { vec[i] = nums1[m]; m--; } else { if(nums1[m] &gt; nums2[n]) { vec[i] = nums1[m]; m--; } else { vec[i] = nums2[n]; n--; } } i--; } if(size%2 == 0) { return (vec[size/2-1]+vec[size/2])/2.0; } else { return vec[size/2]; } return 0.0; }}Double KillAfter I checked with the Discussion part, I learned that this problem is a typical “Binary Search” problem for the keywords “Sorted Array” and “O(log())”.The main idea for this solution is the concept of the median. The median is the middle number of an odd amount of number sorted array, and half of the middle pair numbers’ sum when it is an even amount of number array. The median number can divide the array into two parts, the elements in the left part of the array are less or equal to the median, while the elements in the right part are large or equal to the median.Imagine that if we merge the given two sorted arrays into one sorted array, the elements on the left of the median will consist of the number from both given arrays. So instead of merging two arrays, what we can do is find the right position to divide the given two arrays. In other words, we can divide each sorted array into two parts - the elements in the left part of the array are less than the median and the right part of the array is big than the median. Combining the left parts of the two sorted arrays will form the left part of the merged array, and so is the right part.The question becomes finding the right position to divide the array. Since we know we need to find the middle number to divide the array into the left and right parts. The amount of numbers in the left and right parts is the same. We only need to find the cut position of one array, and then calculate the cut position of the other array. And we will do the binary search to find the correct cut position on the smaller array to speed up the process.Let m be the size of nums1, n be the size of nums2,len = m + n is the total amount of numbers in the two arrays.If len is odd, we assume that the median is on the left part, so there’s one more number on the left than the right.If len is even, the median is half of the sum of the max number in the left part and the min number in the right part, and the number of element in the left and right is the same.left_side_size = (len + 1)/2if len = 3, left_side_size = (3+1)/2 = 2, if len = 2, left_size_size = 1Let i be the number of elements in the left part of nums1j be the number of elements in the right part of nums2j = left_side_size - i;The binary search is to find the correct cut position or the correct size of the left part.We start at the middle of the nums1 array:low = 0;high = m;i = (low+high)/2; // (0+m)/2j = left_side_size - i;To check if we find the right position://i is the size of left part, the index of max number of left part i-1;//the edge case is that, if there’s no element in the nums1 array’s left part, the max number will be set to INT_MINnum1_left_max = (i == 0) ? INT_MIN: nums1[i-1];//the index of min number of the right part is i//the edge case is that, the elements in the nums1 are all in the left side of the merged array, the min number of the right part will be set to INT_MAXnum1_right_min = (i == m_size) ? INT_MAX: nums1[i];num2_left_max = (j == 0) ? INT_MIN: nums2[j-1];num2_right_min = (j == n_size) ? INT_MAX: nums2[j];remember that the elements in the left part is less than the right partto check if we find the correct cut position:num1_left_max &lt;= num2_right_min &amp;&amp; num2_left_max &lt;= num1_right_minif so, we can return the median:if the len is even, the median will be (std::max(num1_left_max,num2_left_max)+std::min(num1_right_min,num2_right_min))/2.0if the len is odd, the median will be (std::max(num1_left_max,num2_left_max))if not, we do the binary search, if num1_left_max &gt; num2_right_min, the num1_left_max should be on the right side of the nums1, in other words, we need to contract the size of the left side of the nums1 array, the possible cut position should be on the left of the current cut position.let high = i - 1;if num2_left_max &gt; num1_right_min, num2_left_max should be on the right side of nums2， we need to expand the size of the left side of the nums1 array, so that the left side of nums2 will be contracted.let low = i + 1;double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) { int m_size = nums1.size(); int n_size = nums2.size(); if(n_size &lt; m_size) { return findMedianSortedArrays(nums2,nums1); } int size = m_size + n_size; //pointer low high for binary search int low = 0; int high = m_size; //i for cut nums1, j for cut nums2 int i = 0; int j = 0; //do binary search on nums1 while(high&gt;=low) { i = (low+high)/2; j = (size+1)/2 - i; int num1_left_max = (i == 0) ? INT_MIN: nums1[i-1]; int num1_right_min = (i == m_size) ? INT_MAX: nums1[i]; int num2_left_max = (j == 0) ? INT_MIN: nums2[j-1]; int num2_right_min = (j == n_size) ? INT_MAX: nums2[j]; //find right i and j to divide two arrays if((num1_left_max &lt;= num2_right_min) &amp;&amp; (num2_left_max &lt;= num1_right_min)) { if((m_size+n_size)%2 == 0) { return (std::max(num1_left_max,num2_left_max)+std::min(num1_right_min,num2_right_min))/2.0; } else { return (double)(std::max(num1_left_max,num2_left_max)); } } //left part of num1 should be contract else if(num1_left_max &gt; num2_right_min) { high = i-1; } else//left part of num1 should be expand { low = i+1; } } return 0.0; }" }, { "title": "Leetcode 0003 Longest Substring Without Repeating Characters", "url": "/posts/leetcode-0003-longest-substring-without-repeating-characters/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-06-20 11:04:00 -0500", "snippet": "Longest Substring Without Repeating Characters problem resultsLongest Substring Without Repeating CharactersWhen I was working, I occasionally heard about the common interview question about the substring, but never bother to think about it. Because, in my daily work, these sorts of questions may not show up and bother me.But this is really interesting. I find out that by solving these questions, I really can learn something interesting and also enhance my logical ability.First BloodBy myself, I only came up with the brute force approach. At first, I tried to record every substring and then compare each other’s lengths to get the size of the longest substring. This approach undoubtedly failed to be accepted.Finally, I got an acceptable answer which is bad of course.{class Solution {public: static int lengthOfLongestSubstring(string s) { if(s.size() == 0) return 0; int j = 0; int count = 0; int max = 0; set&lt;char&gt; substr; for(int i = 0; i &lt; s.size(); i++) { substr.insert(s[i]); j = i + 1; while(j &lt; s.size()) { if(substr.find(s[j]) != substr.end()) { break; } else { substr.insert(s[j]); } j++; } if(max &lt; substr.size()) max = substr.size(); substr.clear(); } return max; }};}Double KillThen, I read the “Solution” to learn the idea of “Sliding Window”. I was like: “Oh, I see, here it is”.After three attempts I finally got an acceptance.{class Solution {public: static int lengthOfLongestSubstring(string s) { if(s.size() &lt;= 1) return s.size(); //sliding window int win_left = 0; int win_right = 0; int max = 0; int cur_str_size = 0; //store visited char unordered_set&lt;char&gt; substr; while(win_right &lt; s.size() &amp;&amp; win_left &lt; s.size()) { char r = s[win_right]; if(substr.find(r) != substr.end()) { substr.erase(s[win_left]); win_left++; } else { substr.insert(r); win_right++; cur_str_size = win_right - win_left; max = max &lt; cur_str_size? cur_str_size: max; } } return max; }};}Sliding Window AlgorithmHere are some notes about the “Sliding Window Algorithm” when solving the longest substring problem.Traversing the given string by a changeable size of the window.First move the right pointer of the window which expands the size of the sliding window, until meeting a repeating character, calculate the size of the current substring, and record the max length.Then move the left pointer of the window which contracts the size of the sliding window, until there is no repeating character in the current substring.The traversing will be stopped until the pointer reaches the end of the given string." }, { "title": "Leetcode 0002 Add Two Numbers", "url": "/posts/leetcode-0002-add-two-numbers/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-06-16 12:08:00 -0500", "snippet": "ADD TWO NUMBERS problem resultsAdd_Two_NumbersFirst BloodI feel very ashamed when solving this problem. How can I not even think about the Integer overflow issue? This should be a very basic trap.My lessons learned from this problem:Use all the test cases…I don’t want to publicize my stupid code…Double KillThe second submission took me almost one hour… Integrating C++ extension with VS Code on my brand new Mac pro. Adding testbed. It’s hard for me to get used to a brand new OS. But I did it!This answer is somehow not very clear and has some redundant “if-else”, could be more clear.{class Solution {public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { ListNode* result_list = new ListNode(); int a = l1-&gt;val; int b = l2-&gt;val; int carry = (a+b)/10; result_list-&gt;val = (a+b)%10; ListNode* ptr = result_list; if(l1-&gt;next != nullptr) { l1 = l1-&gt;next; } else { l1 = nullptr; } if(l2-&gt;next != nullptr) { l2 = l2-&gt;next; } else { l2 = nullptr; } while((l1 != nullptr) || (l2 != nullptr)) { if(l1 != nullptr) { a = l1-&gt;val; } else { a = 0; } if(l2 != nullptr) { b = l2-&gt;val; } else { b = 0; } int r = a + b + carry; ListNode* node = new ListNode(); node-&gt;val = r%10; ptr-&gt;next = node; ptr = ptr-&gt;next; carry = r/10; if(l1 != nullptr &amp;&amp; l1-&gt;next != nullptr) { l1 = l1-&gt;next; } else { l1 = nullptr; } if(l2 != nullptr &amp;&amp; l2-&gt;next != nullptr) { l2 = l2-&gt;next; } else { l2 = nullptr; } } if(carry != 0) { ListNode* node = new ListNode(); node-&gt;val = carry; ptr-&gt;next = node; } return result_list; }};}Triple KillQuadra KillPenta KillOn my last try, return result_list-&gt;next instead of result_list, in order to make the code clear.{ class Solution {public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { ListNode* result_list = new ListNode(); int a = 0; int b = 0; int carry = 0; ListNode* ptr = result_list; while((l1 != nullptr) || (l2 != nullptr)) { a = 0; b = 0; if(l1 != nullptr) { a = l1-&gt;val; } if(l2 != nullptr) { b = l2-&gt;val; } int r = a + b + carry; ListNode* node = new ListNode(); node-&gt;val = r%10; ptr-&gt;next = node; ptr = ptr-&gt;next; carry = r/10; if(l1 != nullptr) { l1 = l1-&gt;next; } if(l2 != nullptr) { l2 = l2-&gt;next; } } if(carry != 0) { ListNode* node = new ListNode(); node-&gt;val = carry; ptr-&gt;next = node; } return result_list-&gt;next; }};}For this problem, or simply for Leetcode, is it ok for us to ignore the memory leaking problem? Like using new inside the class function but never releasing them. Some awful memory of “memory leaking” started to attack me…" }, { "title": "Leetcode 0001 Two Sum", "url": "/posts/Leetcode-0001-Two-Sum/", "categories": "Leetcode-Diary", "tags": "", "date": "2022-06-10 00:00:00 -0500", "snippet": "TWO SUM problem resultsTwo_SumFirst BloodUsing “two loops” to solve the problem is not efficient.{class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { vector&lt;int&gt; indices(2); for(int i = 0; i &lt; nums.size(); i++) { for(int j = 0; j &lt; nums.size(); j++) { if((nums[i] + nums[j] == target) &amp;&amp; (i != j)) { indices[0] = i; indices[1] = j; return indices; } } } return indices; }};}Double Killuse std::map to record “key-value”, in order to speed up the searching process.key: the value of the element in nums arrayvalue: the index of the element in nums array{class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { map&lt;int,int&gt; key_value; //loop array nums, record value_index pair for(int i = 0; i &lt; nums.size(); i++) { key_value[nums[i]] = i; } vector&lt;int&gt; indices(2); //loop array nums, find y for(int i = 0; i &lt; nums.size(); i++) { int y = target - nums[i]; if(key_value.find(y) != key_value.end()) { int y_index = key_value[y]; if(i != y_index) { indices[0] = i; indices[1] = y_index; return indices; } } } return indices; }};}Triple Killsome minor update{ class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { map&lt;int,int&gt; key_value; vector&lt;int&gt; indices(2); int i = 0; for(i = 0; i &lt; nums.size(); i++) { key_value[nums[i]] = i; } //Declare and define y here, avoid define y multiple times inside the loop int y = 0; for(i = 0; i &lt; nums.size(); i++) { y = target - nums[i]; if(key_value.find(y) != key_value.end()) { if(i != key_value[y]) { indices[0] = i; indices[1] = key_value[y]; return indices; } } } return indices; }};}Quadra KillUsing one loop is enough to find the pair{ class Solution {public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) { map&lt;int,int&gt; key_value; vector&lt;int&gt; indices(2); int i = 0; int y = 0; for(i = 0; i &lt; nums.size(); i++) { y = target - nums[i]; if(key_value.find(y) != key_value.end()) { if(i != key_value[y]) { indices[0] = i; indices[1] = key_value[y]; return indices; } } else { key_value[nums[i]] = i; } } return indices; }};}Penta KillUse unordered_map instead of mapSee more detail in The Image is from" }, { "title": "VR Interaction Project", "url": "/posts/vr-interaction-demo/", "categories": "Project-demos", "tags": "", "date": "2021-11-24 18:45:00 -0600", "snippet": " This video demonstrates a user performing a 3D manipulation task in a virtual reality environment.This project was developed through Unity3d and VRTK SDK, on HTC Vive." }, { "title": "Multitouch Interaction Project", "url": "/posts/multi-touch-demo/", "categories": "Project-demos", "tags": "", "date": "2021-11-24 18:38:00 -0600", "snippet": " In the video, a user is performing the 3D manipulation task of a bone through the multi-touch screen. This program was developed through OpenSceneGraph(OSG) engine, C++" }, { "title": "Auto Joint Binding Project", "url": "/posts/auto-joint-binding-demo/", "categories": "Project-demos", "tags": "", "date": "2021-11-24 13:23:00 -0600", "snippet": " This is a trimmed video from Samsung Galaxy Note 10 Launch @ Galaxy Unpacked 2019! This video demonstrates the feature of auto joint binding, motion retargeting, and human tracking." }, { "title": "Welcome to Jekyll!", "url": "/posts/welcome-to-jekyll/", "categories": "jekyll-notes", "tags": "", "date": "2021-11-24 12:29:38 -0600", "snippet": "Some notes about building a site using Jekyll on macOSStep1: Follow the instruction of Jekyll InstallationRemember to relaunch the terminal after configuring your shell to automatically use chrubyStep2: Follow the Quickstart Jekyll Quickstart//create a new Jekyll projectJekyll new GuoJiechang.github.ioStep3: set this file as a git repository and push it to the remoteStep4: Check with My Github PageThen you can see your awesome site!*build your site in local:bundle exec jekyll serveAnd browse to http://localhost:4000Jekyll Cheat SheetHow to add a new post via command?Jekyll-compose is the solution.See more details on Jekyll-compose; Add new post $ bundle exec jekyll post \"My New Post\" Link postJekyll-Linking-to-posts" } ]
